#!/usr/bin/env python3
# -*- coding: utf-8 -*-


import os
import streamlit as st
import requests
import json
import sqlite3
import uuid
import time
import threading
import subprocess
import pandas as pd
import numpy as np
from datetime import date, timedelta
from typing import Optional, List, Dict, Any
from pathlib import Path
from datetime import datetime
from pydantic import BaseModel
import re


DEBUG_LOG_PATH = Path("/tmp/frontend_debug.log")


def debug_log(message: str) -> None:
    """Append debug information to a log file for inspection."""
    try:
        DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        with DEBUG_LOG_PATH.open("a", encoding="utf-8") as fp:
            timestamp = datetime.now().isoformat()
            fp.write(f"{timestamp} {message}\n")
    except Exception:
        # Avoid crashing the app if logging fails
        pass


def sanitize_messages(messages):
    """Ensure each chat message has string content."""
    sanitized = []
    for msg in messages or []:
        if msg is None:
            continue

        if isinstance(msg, dict):
            msg_copy = dict(msg)
        elif hasattr(msg, "model_dump"):
            msg_copy = msg.model_dump()
        elif hasattr(msg, "dict") and callable(getattr(msg, "dict")):
            try:
                msg_copy = msg.dict()
            except TypeError:
                msg_copy = dict(msg)
        else:
            msg_copy = dict(msg)

        content = msg_copy.get("content")

        # Skip messages with null or empty content - OpenAI rejects them
        # Exception: assistant messages with tool_calls can have null content
        has_tool_calls = msg_copy.get("role") == "assistant" and msg_copy.get("tool_calls")
        is_tool_message = msg_copy.get("role") == "tool"

        if (content is None or content == "") and not has_tool_calls and not is_tool_message:
            # Skip empty messages
            continue

        if isinstance(content, (dict, list)):
            try:
                msg_copy["content"] = json.dumps(content)
            except (TypeError, ValueError):
                msg_copy["content"] = str(content)
        elif not isinstance(content, str) and content is not None:
            msg_copy["content"] = str(content)

        sanitized.append(msg_copy)
    return sanitized


TRIP_KEYWORDS = {
    "trip", "travel", "vacation", "itinerary", "holiday",
    "flight", "hotel", "budget", "day", "days", "week", "weeks"
}

CODE_KEYWORDS = {
    "code", "function", "class", "script", "program", "algorithm",
    "implement", "write", "generate", "test", "unit test", "pytest",
    "loop", "array", "list", "json", "binary", "sort", "python",
    "javascript", "typescript", "java", "c++", "c#", "rust", "golang",
    "sql"
}


def detect_mode_from_prompt(text: Optional[str]) -> Optional[str]:
    """Heuristic detection of the intended service mode from free-form text."""
    if not text:
        return None

    lowered = text.lower()

    # Trip heuristics
    if any(keyword in lowered for keyword in TRIP_KEYWORDS):
        if (" to " in lowered and " from " in lowered) or "budget" in lowered or "$" in lowered:
            return "trip"
        if re.search(r"\b\d+\s*(day|days|week|weeks)\b", lowered):
            return "trip"

    # Code heuristics
    if any(keyword in lowered for keyword in CODE_KEYWORDS):
        return "code"

    return None


MODE_ACTIVATION_MESSAGES = {
    "rag": """ğŸ“š **RAG Q&A Mode Activated**\n\nI'll help you search and answer questions from the document collection.\n\n**Examples:**\n- \"Who wrote DADDY TAKE ME SKATING?\"\n- \"Tell me about American frontier history\"\n- 'Sir roberts fortune a novel', for what purpose he was confident of his own powers of cheating the uncle, and managing?\n\nType 'q'(quit) to exit this mode.\n""",
    "trip": """âœˆï¸ **Trip Planning Mode Activated**\n\nI'll help you plan your perfect trip! I need to collect four key pieces of information:\n- ğŸ“ Where do you want to go?\n- ğŸ›« Where are you leaving from?\n- ğŸ“… How many days?\n- ğŸ’° What's your budget?\n\n**Examples:**\n- \"I want to go to Tokyo from Auckland for 5 days with $2000\"\n- \"Plan a trip to Paris, 1 week, budget 3000 NZD, from Wellington\"\n\nType 'q'(quit) to exit this mode.\n""",
    "code": """ğŸ’» **Code Generation Mode Activated**\n\nI'll generate code with automated tests and self-healing capabilities!\n- Type 'q'(quit) to exit this mode.\n* Examples:\n- 1 \"Write a function to check if a number is prime\"\n- 2 \"Create a binary search algorithm in Python\"\n- 3 \"Implement a quick sort in JavaScript\"\n- 4 \"Classic Problem: â€œ\n""",
}


def maybe_append_mode_intro(mode: str) -> None:
    if "first_activation" not in st.session_state:
        return
    if st.session_state.first_activation.get(mode):
        message = MODE_ACTIVATION_MESSAGES.get(mode)
        if message:
            append_chat_history("assistant", message)
        st.session_state.first_activation[mode] = False

# import model and services
try:
    from dotenv import load_dotenv

    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)
except ImportError as e:
    st.error(f"Import error: {e}")


class TripConstraints(BaseModel):
    """Local copy of trip constraint schema used for UI state."""
    budget: Optional[float] = None
    currency: Optional[str] = "USD"
    days: Optional[int] = None
    origin_city: Optional[str] = None
    destination_city: Optional[str] = None
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    preferences: Optional[List[str]] = None


CITY_TO_COUNTRY = {
    "auckland": "new zealand",
    "wellington": "new zealand",
    "christchurch": "new zealand",
    "sydney": "australia",
    "melbourne": "australia",
    "beijing": "china",
    "shanghai": "china",
    "guangzhou": "china",
    "shenzhen": "china",
    "hong kong": "china",
    "singapore": "singapore",
    "tokyo": "japan",
    "osaka": "japan",
    "kyoto": "japan",
    "new york": "united states",
    "los angeles": "united states",
    "san francisco": "united states",
    "london": "united kingdom",
    "paris": "france",
    "berlin": "germany",
    "dubai": "united arab emirates",
    "bangkok": "thailand",
    "delhi": "india",
    "mumbai": "india",
    "rome": "italy",
    "barcelona": "spain",
}

COUNTRY_TO_CURRENCY = {
    "new zealand": "NZD",
    "australia": "AUD",
    "china": "CNY",
    "singapore": "SGD",
    "japan": "JPY",
    "united states": "USD",
    "united kingdom": "GBP",
    "france": "EUR",
    "germany": "EUR",
    "united arab emirates": "AED",
    "thailand": "THB",
    "india": "INR",
    "italy": "EUR",
    "spain": "EUR",
}


def _normalize_city(value: Optional[str]) -> Optional[str]:
    if not value:
        return None
    return value.strip().lower()


def infer_currency_from_origin(origin_city: Optional[str]) -> Optional[str]:
    norm_city = _normalize_city(origin_city)
    if not norm_city:
        return None
    country = CITY_TO_COUNTRY.get(norm_city)
    if not country:
        return None
    return COUNTRY_TO_CURRENCY.get(country)


def apply_origin_currency(constraints: TripConstraints) -> None:
    inferred = infer_currency_from_origin(constraints.origin_city)
    if inferred:
        constraints.currency = inferred


BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8888").rstrip("/")


MODEL_PRICING = {
    "gpt-3.5-turbo": {"prompt": 0.0015, "completion": 0.0020},
    "gpt-3.5-turbo-0125": {"prompt": 0.0015, "completion": 0.0020},
    "Gpt4o": {"prompt": 0.005, "completion": 0.015},
    "Gpt4o-mini": {"prompt": 0.00015, "completion": 0.0006},
}

CURRENCY_TO_NZD = {
    "NZD": 1.0,
    "USD": 1.65,
    "AUD": 1.08,
    "GBP": 2.15,
    "EUR": 1.82,
    "JPY": 0.011,
    "SGD": 1.31,
    "CNY": 0.24,
    "HKD": 0.21,
    "THB": 0.05,
    "INR": 0.02,
    "AED": 0.45,
}


def convert_currency(amount: Optional[float], source: Optional[str], target: str) -> Optional[float]:
    """Convert currencies using the same baseline as the backend."""
    if amount is None or source is None:
        return None

    source_rate = CURRENCY_TO_NZD.get(source.upper())
    target_rate = CURRENCY_TO_NZD.get(target.upper())
    if not source_rate or not target_rate or source_rate <= 0 or target_rate <= 0:
        return None

    return amount * source_rate / target_rate


def clean_text_lines(text: Optional[str]) -> str:
    """Remove lines that lack alphanumeric characters."""
    if not text:
        return ""
    lines = text.splitlines()
    filtered = [line for line in lines if re.search(r"[A-Za-z0-9]", line)]
    return "\n".join(filtered).strip()


def append_chat_history(role: str, content: str) -> None:
    """Append a message to chat history with optional cleaning."""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if role == "assistant":
        content = clean_text_lines(content)
        if not content:
            return
    st.session_state.messages.append({"role": role, "content": content})


def check_service_health(url: str, timeout: float = 5.0) -> bool:
    try:
        response = requests.get(url, timeout=timeout)
        return 200 <= response.status_code < 400
    except Exception:
        return False


def format_model_label(value: Optional[str]) -> str:
    """Produce a compact label for model paths or URLs."""
    if not value:
        return "â€”"
    value = str(value)
    lowered = value.strip().lower()
    if lowered in {"remote", "api"}:
        return "Remote"
    if lowered == "disabled":
        return "Disabled"
    if value.startswith("http://") or value.startswith("https://"):
        return value
    name = Path(value).name
    parent = Path(value).parent.name if Path(value).parent else ""
    if parent and name:
        return f"{parent}/{name}"
    return name or value


def get_rag_server_config() -> Dict[str, Any]:
    """Fetch RAG backend configuration once per session."""
    if "rag_server_config" not in st.session_state:
        try:
            resp = requests.get(f"{BACKEND_URL}/api/rag/config", timeout=5)
            resp.raise_for_status()
            st.session_state.rag_server_config = resp.json()
        except Exception as exc:
            st.session_state.rag_server_config = {
                "models": {},
                "reranker_options": ["auto"],
                "limits": {
                    "vector_min": 6,
                    "vector_max": 20,
                    "content_char_min": 150,
                    "content_char_max": 1000,
                    "content_char_default": 300,
                },
            }
            st.session_state.rag_config_error = str(exc)
    return st.session_state.rag_server_config


def wait_for_backend_ready(timeout: float = 30.0, poll_interval: float = 1.0) -> bool:
    """Poll backend health until ready (or timeout)."""
    deadline = time.time() + timeout
    while time.time() < deadline:
        try:
            resp = requests.get(f"{BACKEND_URL}/health", timeout=2)
            if resp.ok:
                return True
        except requests.RequestException:
            time.sleep(poll_interval)
    return False


def fetch_seed_status() -> Optional[Dict[str, Any]]:
    """Retrieve current Qdrant seed progress from the backend."""
    try:
        resp = requests.get(f"{BACKEND_URL}/api/rag/seed-status", timeout=5)
        if resp.ok:
            return resp.json()
    except requests.RequestException:
        return None
    return None


def load_warmup_questions() -> List[str]:
    """
    Load warmup questions from RAG evaluation files.
    Extracts 5 questions from each of the 3 eval files for a total of 15 questions.
    Falls back to generic questions if files are not found.
    """
    eval_files = [
        "../data/rag_eval_keyword.json",
        "../data/rag_eval_metadata.json",
        "../data/rag_eval_semantic.json"
    ]

    warmup_questions = []

    for file_path in eval_files:
        try:
            full_path = Path(__file__).parent / file_path
            if not full_path.exists():
                continue

            with open(full_path) as f:
                data = json.load(f)

            # Extract questions (format might vary)
            if isinstance(data, list):
                questions = [item.get('question', item.get('query', '')) for item in data[:15]]
            elif isinstance(data, dict) and 'questions' in data:
                questions = data['questions'][:15]
            else:
                # Try to find questions in nested structure
                questions = []
                for key, value in list(data.items())[:15]:
                    if isinstance(value, dict) and 'question' in value:
                        questions.append(value['question'])
                    elif isinstance(value, dict) and 'query' in value:
                        questions.append(value['query'])

            warmup_questions.extend([q for q in questions if q])
        except Exception:
            # Silently skip if file can't be loaded
            continue

    # If we got at least 15 real questions, use them
    if len(warmup_questions) >= 15:
        return warmup_questions[:15]

    # Otherwise fall back to generic questions (5 total)
    return [
        "What is Shaun O'Day of Ireland about?",
        "Who wrote Shaun O'Day of Ireland?.",
        "What is Musical Myths and Facts about?",
        "According to 'the-ink-stain-tache-d-encre-complete', what happens when his moral studies of provincial life under the form of\nnovels and romances became appreciated?",
        "In 'dorothy-south-a-love-story-of-virginia-just-before-the-war', why he sat down not?",
    ]


def estimate_completion_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:
    pricing = MODEL_PRICING.get(model, MODEL_PRICING["gpt-3.5-turbo"])
    prompt_cost = (prompt_tokens / 1000) * pricing["prompt"]
    completion_cost = (completion_tokens / 1000) * pricing["completion"]
    return prompt_cost + completion_cost


def render_rag_controls(target) -> None:
    """Render RAG Dashboard inside the given container."""
    target.markdown("### ğŸ”§ RAG Controls")

    if st.session_state.mode != "rag":
        target.info("Activate RAG mode to configure retrieval settings.")
        return

    if "rag_config_error" in st.session_state:
        target.caption(f"âš ï¸ Using default limits ({st.session_state.rag_config_error})")

    vector_value = target.slider(
        "Vector Candidate Limit",
        min_value=vector_min,
        max_value=vector_max,
        value=int(st.session_state.rag_vector_limit),
        help="Number of candidate vectors retrieved before reranking",
        key="rag_vector_limit_slider",
    )
    st.session_state.rag_vector_limit = vector_value

    content_value = target.slider(
        "Chunk Character Limit",
        min_value=content_min,
        max_value=content_max,
        value=int(st.session_state.rag_content_limit),
        step=50,
        help="Truncate chunk text to control reranker input length",
        key="rag_content_limit_slider",
    )
    st.session_state.rag_content_limit = content_value

    labels_map = {
        "auto": "Auto (adaptive)",
        "primary": "Primary (BGE-m3-int8)",
        "fallback": "Fallback (MiniLM-int8)",
        "remote": "Remote",
        "custom": "Custom",
    }
    displayed_options = [labels_map.get(opt, opt.title()) for opt in reranker_options]
    current_choice = st.session_state.rag_reranker_choice
    if current_choice not in reranker_options:
        current_choice = reranker_options[0]
    current_index = reranker_options.index(current_choice)

    selected_label = target.selectbox(
        "Reranker",
        displayed_options,
        index=current_index,
        help="Choose which reranker to apply for this session",
        key="rag_reranker_choice_select",
    )
    reverse_map = {labels_map.get(opt, opt.title()): opt for opt in reranker_options}
    new_reranker_choice = reverse_map[selected_label]

    # Detect reranker change and trigger warm-up
    if "rag_last_reranker" not in st.session_state:
        st.session_state.rag_last_reranker = None

    if st.session_state.rag_last_reranker is not None and st.session_state.rag_last_reranker != new_reranker_choice:
        # Reranker changed, perform warm-up with 15 queries from eval data
        with st.spinner(f"ğŸ”¥ Warming up {selected_label} reranker (15 queries)..."):
            try:
                # Load real eval questions for warm-up
                warmup_questions = load_warmup_questions()

                for i, question in enumerate(warmup_questions, 1):
                    warmup_response = requests.post(
                        f"{BACKEND_URL}/api/rag/ask",
                        json={
                            "question": question,
                            "top_k": 3,
                            "include_timings": True,  # Use same code path as real queries
                            "reranker": new_reranker_choice,
                            "vector_limit": 5,
                            "content_char_limit": 300
                        },
                        timeout=30
                    )
                st.success(f"âœ… {selected_label} reranker fully warmed up!")
                time.sleep(0.5)
            except Exception as e:
                st.warning(f"âš ï¸ Warm-up failed: {e}")

    st.session_state.rag_reranker_choice = new_reranker_choice
    st.session_state.rag_last_reranker = new_reranker_choice

    model_info = rag_server_config.get("models", {})
    selected_label = labels_map.get(st.session_state.rag_reranker_choice, st.session_state.rag_reranker_choice.title())
    target.caption(
        f"Embedding: `{format_model_label(model_info.get('embedding'))}`  "
        f"â€¢ Default reranker: `{format_model_label(model_info.get('reranker_current'))}`  "
        f"â€¢ Currently selected: `{selected_label}`"
    )

    summary = st.session_state.get("rag_last_summary")
    if summary:
        timings = summary.get("timings") or {}
        models = summary.get("models") or {}
        target.markdown("**Last Query Timing**")
        columns = target.columns(5)
        columns[0].metric("Embed", f"{timings.get('embed_ms', 0.0):.1f}ms")
        columns[1].metric("Vector", f"{timings.get('vector_ms', 0.0):.1f}ms")
        columns[2].metric("Rerank", f"{timings.get('rerank_ms', 0.0):.1f}ms")
        columns[3].metric("LLM", f"{timings.get('llm_ms', 0.0):.1f}ms")
        columns[4].metric("Total", f"{timings.get('end_to_end_ms', 0.0):.1f}ms")

        extra = []
        if summary.get("vector_limit") is not None:
            extra.append(f"vector {summary['vector_limit']}")
        if summary.get("content_limit") is not None:
            extra.append(f"chars {summary['content_limit']}")
        if summary.get("reranker_mode"):
            extra.append(f"mode {summary['reranker_mode']}")
        if extra:
            target.caption(" | ".join(extra))

        usage = summary.get("token_usage") or {}
        cost_usd = float(summary.get("token_cost_usd") or 0.0)
        if usage or cost_usd:
            target.markdown("**Last Query Tokens**")
            token_cols = target.columns(4)
            token_cols[0].metric("Prompt", int(usage.get("prompt", 0) or 0))
            token_cols[1].metric("Completion", int(usage.get("completion", 0) or 0))
            token_cols[2].metric("Total", int(usage.get("total", 0) or 0))
            token_cols[3].metric("Token Cost (USD)", f"${cost_usd:.4f}")

        target.markdown("**Models Used (Last Query)**")
        target.markdown(
            f"- Embedding: `{format_model_label(models.get('embedding'))}`\n"
            f"- Reranker: `{format_model_label(models.get('reranker'))}`\n"
            f"- LLM: `{models.get('llm', 'â€”')}`"
        )


# =====================================================================
# SessionManager 
# =====================================================================

class SessionManager:
    """ç®¡ç†ä¼šè¯çŠ¶æ€å’Œæ¶ˆæ¯å†å²çš„ SQLite æ•°æ®åº“"""

    def __init__(self, db_path: str = "chat_sessions.sqlite3"):
        self.db_path = db_path
        self._ensure_tables()

    def _ensure_tables(self):
        """ç¡®ä¿æ•°æ®åº“è¡¨å­˜åœ¨"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS session_state (
                    session_id TEXT PRIMARY KEY,
                    constraints_json TEXT,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS session_message (
                    session_id TEXT,
                    role TEXT,
                    content TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()

    def load_constraints(self, session_id: str) -> Optional[TripConstraints]:
        """ä»æ•°æ®åº“åŠ è½½çº¦æŸ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                "SELECT constraints_json FROM session_state WHERE session_id = ?",
                (session_id,)
            )
            row = cursor.fetchone()
            if row and row[0]:
                try:
                    constraints = TripConstraints.model_validate_json(row[0])
                    apply_origin_currency(constraints)
                    return constraints
                except Exception:
                    return None
        return None

    def save_constraints(self, session_id: str, constraints: TripConstraints):
        """ä¿å­˜çº¦æŸåˆ°æ•°æ®åº“"""
        constraints_json = constraints.model_dump_json()
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO session_state (session_id, constraints_json, updated_at)
                VALUES (?, ?, CURRENT_TIMESTAMP)
            """, (session_id, constraints_json))
            conn.commit()

    def add_message(self, session_id: str, role: str, content: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT INTO session_message (session_id, role, content) VALUES (?, ?, ?)",
                (session_id, role, content)
            )
            conn.commit()

    def load_messages(self, session_id: str, limit: int = 50) -> List[Dict[str, str]]:
        """åŠ è½½æ¶ˆæ¯å†å²"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute(
                """SELECT role, content FROM session_message
                   WHERE session_id = ?
                   ORDER BY created_at DESC LIMIT ?""",
                (session_id, limit)
            )
            messages = [
                {"role": row[0], "content": row[1] or ""}  # Convert None to empty string
                for row in cursor.fetchall()
                if row[1]  # Skip messages with null content
            ]
            return list(reversed(messages))


# =====================================================================
# çº¦æŸæå–å‡½æ•° 
# =====================================================================

def parse_constraints_from_text(text: str, existing: Optional[TripConstraints] = None) -> TripConstraints:
    """ä»ç”¨æˆ·è¾“å…¥ä¸­æå–çº¦æŸä¿¡æ¯ï¼ˆæ­£åˆ™æ–¹å¼ï¼‰"""
    import re

    # Use a copy of existing constraints to avoid modifying the original
    constraints = existing.model_copy(deep=True) if existing else TripConstraints()

    # æå–é¢„ç®—
    money_patterns = [
        (r'(?P<currency>NZ)\s*\$\s*(?P<amount>\d+)', 'NZD'),
        (r'(?P<currency>NZD)\s*(?P<amount>\d+)', 'NZD'),
        (r'budget\s*(?:is|of)?\s*(?P<currency_word>NZD|NZ)\s*\$?\s*(?P<amount>\d+)', 'NZD'),
        (r'(?P<currency>USD|US)\s*\$?\s*(?P<amount>\d+)', 'USD'),
        (r'(?P<currency>AUD|AU)\s*\$?\s*(?P<amount>\d+)', 'AUD'),
        (r'Â£\s*(?P<amount>\d+)', 'GBP'),
        (r'â‚¬\s*(?P<amount>\d+)', 'EUR'),
        (r'budget\s*(?:is|of)?\s*\$?\s*(?P<amount>\d+)', None),
        (r'under\s*\$?\s*(?P<amount>\d+)', None),
        (r'\$\s*(?P<amount>\d+)', None),
        (r'with\s+(?P<amount>\d+)', None),
        (r'\b(?P<amount>\d{2,5})\b', None),
    ]

    for pattern, forced_currency in money_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            groups = match.groupdict()
            amount = groups.get("amount")
            if amount:
                constraints.budget = float(amount)
                if forced_currency:
                    constraints.currency = forced_currency
                else:
                    cur = groups.get("currency") or groups.get("currency_word")
                    if cur:
                        cur_upper = cur.upper()
                        if "NZ" in cur_upper:
                            constraints.currency = "NZD"
                        elif "US" in cur_upper:
                            constraints.currency = "USD"
                        elif "AU" in cur_upper:
                            constraints.currency = "AUD"
                    # If no currency in text, keep existing currency or default to USD
                    elif not constraints.currency:
                        # Prefer to keep the currency from existing constraints
                        if existing and existing.currency:
                            constraints.currency = existing.currency
                        else:
                            constraints.currency = "USD"
                break

    # æå–å¤©æ•°
    day_patterns = [
        (r'(\d+)[-\s]days?', 1),
        (r'for\s+(\d+)\s+days?', 1),
        (r'(\d+)\s+day\s+trip', 1),
        (r'(\d+)[-\s]weeks?', 7),
        (r'for\s+(\d+)\s+weeks?', 7),
    ]
    for pattern, multiplier in day_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            constraints.days = int(match.group(1)) * multiplier
            break

    # æå–ç›®çš„åœ°å’Œå‡ºå‘åœ°
    common_cities = ['Auckland', 'Wellington', 'Christchurch', 'London', 'Paris', 'Tokyo',
                     'New York', 'Sydney', 'Singapore', 'Rome', 'Barcelona', 'Beijing',
                     'Shanghai', 'Guangzhou', 'Shenzhen', 'Hong Kong', 'Taipei', 'Zhuhai']

    for city in common_cities:
        if city.lower() in text.lower():
            if re.search(rf'\b(?:to|visit|in|go)\s+{city}', text, re.IGNORECASE):
                constraints.destination_city = city
            elif re.search(rf'\b(?:from|leaving|depart)\s+{city}', text, re.IGNORECASE):
                constraints.origin_city = city
            elif not constraints.destination_city and not constraints.origin_city:
                constraints.destination_city = city

    # é€šç”¨ to/from æ¨¡å¼
    to_patterns = [
        r'\b(?:to|visit|visiting|in)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)',
        r'\bgo\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)',
    ]
    for pattern in to_patterns:
        to_match = re.search(pattern, text)
        if to_match and not constraints.destination_city:
            city_name = to_match.group(1).strip()
            if city_name.lower() not in ['with', 'for', 'from', 'days', 'day', 'week', 'weeks']:
                constraints.destination_city = city_name
                break

    from_patterns = [
        r'\b(?:from|leaving|depart(?:ing)?)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)',
        r'\b([A-Z][a-z]+)\s+to\s+[A-Z]',
    ]
    for pattern in from_patterns:
        from_match = re.search(pattern, text)
        if from_match and not constraints.origin_city:
            city_name = from_match.group(1).strip()
            if city_name.lower() not in ['with', 'for', 'to', 'days', 'day', 'week', 'weeks', 'go', 'plan']:
                constraints.origin_city = city_name
                break

    apply_origin_currency(constraints)
    return constraints

# 1) æ”¾åœ¨å·¥å…·å‡½æ•°åŒº
def _is_quit(msg: Optional[str]) -> bool:
    return bool(msg) and msg.strip().lower() in {"q", "quit", "exit", "cancel"}

def has_potential_missing_info(text: str) -> bool:
    """åˆ¤æ–­æ–‡æœ¬ä¸­æ˜¯å¦å¯èƒ½åŒ…å«æœªæå–çš„åŸå¸‚/åœ°ç‚¹ä¿¡æ¯"""
    import re

    capitalized_words = re.findall(r'\b[A-Z][a-z]+\b', text)
    common_words = {'I', 'The', 'A', 'An', 'My', 'We', 'He', 'She', 'It', 'They', 'This', 'That', 'From', 'To'}
    potential_cities = [w for w in capitalized_words if w not in common_words]

    if len(potential_cities) > 0:
        return True

    words = text.strip().split()
    if len(words) == 1 and words[0].isalpha() and len(words[0]) >= 3:
        common_lowercase = {'from', 'to', 'yes', 'no', 'ok', 'sure', 'go', 'the', 'and', 'or', 'but', 'for', 'with'}
        if words[0].lower() not in common_lowercase:
            return True

    return False


def llm_extract_constraints(text: str, existing: Optional[TripConstraints] = None) -> TripConstraints:
    """ä½¿ç”¨ LLM æå–çº¦æŸä¿¡æ¯ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰"""
    try:
        from openai import OpenAI

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.info("ğŸ’¬ General AI Assistant Mode (local fallback)")
            local_reply = (
                "å—¨ï¼æˆ‘å·²ç»æ”¶åˆ°ä½ çš„æ¶ˆæ¯å•¦ ğŸ‘‹\n\n"
                "ç›®å‰æ²¡æœ‰é…ç½® OPENAI_API_KEYï¼Œæ‰€ä»¥å…ˆç”¨æœ¬åœ°å…œåº•å›å¤ã€‚\n"
                "ä½ å¯ä»¥ç»§ç»­é—®æˆ‘ï¼š\n"
                "- è¾“å…¥ â€œtrip â€¦â€ è®©æˆ‘è¿›å…¥è¡Œç¨‹è§„åˆ’\n"
                "- è¾“å…¥ â€œrag â€¦â€ è®©æˆ‘æŸ¥æ–‡æ¡£\n"
                "- è¾“å…¥ â€œcode â€¦â€ è®©æˆ‘å†™ä»£ç \n"
            )
            st.markdown(local_reply)
            append_chat_history("assistant", local_reply)
            return existing or TripConstraints()

        client = OpenAI(
            api_key=api_key,
            base_url=os.getenv("OPENAI_BASE_URL")
        )

        existing_json = existing.model_dump() if existing else {}

        missing_fields = []
        if not existing_json.get('destination_city'):
            missing_fields.append('destination')
        if not existing_json.get('origin_city'):
            missing_fields.append('origin')

        context_hint = ""
        if missing_fields:
            context_hint = f"\nContext: We are asking the user about: {', '.join(missing_fields)}"

        prompt = f"""Extract trip planning information from the user's message. Return ONLY a JSON object.

User message: "{text}"{context_hint}

Current information: {json.dumps(existing_json, ensure_ascii=False)}

Extract and return JSON with these fields (keep existing values if not mentioned):
{{
    "destination_city": "destination city name or null",
    "origin_city": "origin city name or null",
    "days": number of days or null,
    "budget": number or null,
    "currency": "USD/NZD/AUD/GBP/EUR or null"
}}

Rules:
- Any city name worldwide is valid (e.g., Zhuhai, Auckland, Beijing, Macau, etc.)
- If user provides a single city name when origin is missing, assume it's the origin city
- If user says "1 week", convert to days: 7
- Default currency is USD if $ is mentioned without specification
- Return null for missing information
- ONLY return the JSON object, no explanation"""

        messages = sanitize_messages([
            {"role": "user", "content": prompt}
        ])

        response = client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo-0125"),
            messages=messages,
            temperature=0,
            max_tokens=200
        )

        result_text = response.choices[0].message.content.strip()

        if result_text.startswith("```"):
            result_text = result_text.split("```")[1]
            if result_text.startswith("json"):
                result_text = result_text[4:]

        extracted = json.loads(result_text)

        constraints = existing or TripConstraints()
        if extracted.get("destination_city"):
            constraints.destination_city = extracted["destination_city"]
        if extracted.get("origin_city"):
            constraints.origin_city = extracted["origin_city"]
        if extracted.get("days"):
            constraints.days = int(extracted["days"])
        if extracted.get("budget"):
            constraints.budget = float(extracted["budget"])
        if extracted.get("currency"):
            constraints.currency = extracted["currency"]

        apply_origin_currency(constraints)
        return constraints

    except Exception as e:
        return existing or TripConstraints()


def extract_constraints_hybrid(text: str, existing: Optional[TripConstraints] = None) -> tuple:
    """æ··åˆæå–ï¼šå…ˆç”¨æ­£åˆ™ï¼Œæå–ä¸åˆ°å…³é”®ä¿¡æ¯æ—¶ç”¨ LLM å…œåº•"""
    base_constraints = existing or TripConstraints()
    old_constraints = base_constraints.model_copy(deep=True)
    constraints = parse_constraints_from_text(text, base_constraints)

    new_dest = constraints.destination_city != old_constraints.destination_city
    new_origin = constraints.origin_city != old_constraints.origin_city
    has_new_city = new_dest or new_origin

    if not has_new_city and has_potential_missing_info(text):
        words = text.strip().split()
        if len(words) == 1 and words[0].isalpha():
            capitalized = words[0].capitalize()
            constraints = parse_constraints_from_text(capitalized, existing)
            new_dest = constraints.destination_city != old_constraints.destination_city
            new_origin = constraints.origin_city != old_constraints.origin_city
            if new_dest or new_origin:
                return constraints, False

        constraints = llm_extract_constraints(text, constraints)
        return constraints, True

    return constraints, False


def check_constraints_complete(constraints: TripConstraints) -> tuple:
    """æ£€æŸ¥å››è¦ç´ æ˜¯å¦å®Œæ•´"""
    missing = []
    if not constraints.destination_city:
        missing.append("destination")
    if not constraints.origin_city:
        missing.append("origin")
    if not constraints.days or constraints.days < 1:
        missing.append("days")
    if not constraints.budget or constraints.budget < 0:
        missing.append("budget")

    return len(missing) == 0, missing


def validate_constraints(constraints: TripConstraints) -> tuple:
    """éªŒè¯çº¦æŸæ˜¯å¦åˆç†"""
    issues = []

    if constraints.days and (constraints.days < 1 or constraints.days > 30):
        issues.append(f"Trip duration {constraints.days} days seems unrealistic (should be 1-30)")

    if constraints.budget is not None:
        currency = constraints.currency or infer_currency_from_origin(constraints.origin_city) or "USD"
        budget_value = float(constraints.budget)
        budget_usd = convert_currency(budget_value, currency, "USD")
        if currency.upper() == "USD" and budget_usd is None:
            budget_usd = budget_value

        if budget_usd is not None:
            if budget_usd < 150:
                issues.append(
                    f"Budget {currency} {budget_value:.0f} seems too low (â‰ˆ USD {budget_usd:.0f})"
                )
            elif budget_usd > 25000:
                issues.append(
                    f"Budget {currency} {budget_value:.0f} seems exceptionally high (â‰ˆ USD {budget_usd:.0f})"
                )

    if constraints.origin_city and constraints.destination_city:
        if constraints.origin_city.lower().strip() == constraints.destination_city.lower().strip():
            issues.append("Origin and destination are the same")

    return len(issues) == 0, issues


def format_constraints_summary(constraints: TripConstraints) -> str:
    """æ ¼å¼åŒ–çº¦æŸæ‘˜è¦"""
    lines = []
    if constraints.destination_city:
        lines.append(f"ğŸ“ Destination: {constraints.destination_city}")
    if constraints.origin_city:
        lines.append(f"ğŸ›« Origin: {constraints.origin_city}")
    if constraints.days:
        lines.append(f"ğŸ“… Duration: {constraints.days} days")
    if constraints.budget is not None:
        currency_label = constraints.currency or infer_currency_from_origin(constraints.origin_city) or "â€”"
        lines.append(f"ğŸ’° Budget: {currency_label} {constraints.budget:.2f}")
    if constraints.preferences:
        lines.append(f"â¤ï¸ Preferences: {', '.join(constraints.preferences)}")

    return "\n".join(lines) if lines else "No information collected yet"


def render_trip_plan_summary(plan: Dict[str, Any]) -> None:
    """Display a compact summary of the most recent trip plan."""
    itinerary = plan.get("itinerary") or {}
    destination = itinerary.get("destination", "Unknown")
    currency = (itinerary.get("currency") or "-").upper()
    total_cost = itinerary.get("total_cost")
    total_cost_usd = itinerary.get("total_cost_usd")
    days = len(itinerary.get("daily_plan") or [])

    st.markdown(f"**ğŸ“ Destination:** {destination}")
    st.markdown(f"**ğŸ“… Duration:** {days or 'â€”'} days")
    if total_cost is not None:
        cost_line = f"**ğŸ’µ Total:** {currency} {total_cost:.2f}"
        if total_cost_usd:
            cost_line += f" (â‰ˆ USD {total_cost_usd:.2f})"
        st.markdown(cost_line)

    flights = itinerary.get("flights") or []
    if flights:
        st.markdown("**âœˆï¸ Flights:**")
        for flight in flights[:2]:
            airline = flight.get("airline", "Unknown airline")
            number = flight.get("flight_number", "N/A")
            price = flight.get("price")
            flight_currency = (flight.get("currency") or currency).upper()
            line = f"- {airline} {number}"
            if price is not None:
                line += f" â€” {flight_currency} {price:.2f}"
            st.markdown(line)

    cost_breakdown = itinerary.get("cost_breakdown") or {}
    if cost_breakdown:
        st.markdown("**Cost Breakdown:**")
        for label, key in [("Flights", "flights"), ("Accommodation", "accommodation"), ("Meals", "meals"), ("Transport & Activities", "other")]:
            amount = cost_breakdown.get(key)
            if amount:
                st.markdown(f"- {label}: {currency} {float(amount):.2f}")

    token_usage = plan.get("llm_token_usage") or {}
    token_cost = plan.get("llm_cost_usd") or 0.0
    if token_usage or token_cost:
        st.markdown("**LLM Token Usage:**")
        col_a, col_b, col_c, col_d = st.columns(4)
        col_a.metric("Prompt", int(token_usage.get("prompt", 0) or 0))
        col_b.metric("Completion", int(token_usage.get("completion", 0) or 0))
        col_c.metric("Total", int(token_usage.get("total", (token_usage.get("prompt", 0) or 0) + (token_usage.get("completion", 0) or 0))))
        col_d.metric("Token Cost", f"${float(token_cost):.4f}")

    currency_note = itinerary.get("currency_note")
    if currency_note:
        st.caption(currency_note)

    fx_rates = itinerary.get("fx_rates") or {}
    if fx_rates:
        fx_lines = []
        for key, value in fx_rates.items():
            if not isinstance(value, (int, float)):
                continue
            if "->" in key:
                src, tgt = key.split("->", 1)
                fx_lines.append(f"1 {src.upper()} â‰ˆ {value:.4f} {tgt.upper()}")
            else:
                fx_lines.append(f"1 {key.upper()} â‰ˆ {value:.4f} {currency}")
        if fx_lines:
            st.caption("FX rates: " + "; ".join(fx_lines))

# =====================================================================
# Streamlit UI é…ç½®
# =====================================================================

st.set_page_config(
    page_title="AI Assistant",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.markdown(
    """
    <style>
        div[data-testid="stMetricValue"] {
            font-size: 1.6rem;
        }
        div[data-testid="stMetricLabel"] {
            font-size: 0.85rem;
        }
        div[data-testid="stMetricDelta"] {
            font-size: 0.75rem;
        }
    </style>
    """,
    unsafe_allow_html=True,
)

# åˆå§‹åŒ– session state
if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())[:8]

if "messages" not in st.session_state:
    st.session_state.messages = []

if "mode" not in st.session_state:
    st.session_state.mode = "general"  # Default to general assistant mode

if "trip_constraints" not in st.session_state:
    st.session_state.trip_constraints = TripConstraints()

if "session_mgr" not in st.session_state:
    st.session_state.session_mgr = SessionManager()

if "first_activation" not in st.session_state:
    st.session_state.first_activation = {"rag": True, "trip": True, "code": True}

if "awaiting_confirmation" not in st.session_state:
    st.session_state.awaiting_confirmation = False

if "pending_prompt" not in st.session_state:
    st.session_state.pending_prompt = None

# æŒ‡æ ‡è·Ÿè¸ª
if "metrics_history" not in st.session_state:
    st.session_state.metrics_history = {
        "timestamps": [],
        "latencies": [],
        "prompt_tokens": [],
        "completion_tokens": [],
        "costs": [],
        "services": []  # "rag", "trip", "code"
    }

if "rag_metrics" not in st.session_state:
    st.session_state.rag_metrics = {
        "embed_times": [],
        "rerank_times": [],
        "retrieval_times": [],
        "confidences": []
    }

if "agent_stats" not in st.session_state:
    st.session_state.agent_stats = {
        "success": 0,
        "failure": 0,
        "partial": 0
    }

if "trip_last_plan" not in st.session_state:
    st.session_state.trip_last_plan = None

if "rag_stats" not in st.session_state:
    st.session_state.rag_stats = {
        "total_queries": 0,
        "total_retrieval_ms": 0.0,
        "total_rerank_ms": 0.0,
        "total_llm_ms": 0.0,
        "total_tokens": 0,
        "total_cost_usd": 0.0,
        "avg_confidence": 0.0,
        "primary_reranker_count": 0,
        "fallback_reranker_count": 0,
    }

if "code_stats" not in st.session_state:
    st.session_state.code_stats = {
        "total_runs": 0,
        "passes": 0,
        "failures": 0,
        "errors": 0,
        "total_latency_ms": 0.0,
        "total_tokens": 0,
        "total_cost_usd": 0.0,
        "history": [],
    }

if "code_show_samples" not in st.session_state:
    st.session_state.code_show_samples = True

if "code_show_samples_prev" not in st.session_state:
    st.session_state.code_show_samples_prev = True

if "code_last_request" not in st.session_state:
    st.session_state.code_last_request: Optional[Dict[str, Any]] = None

if "code_pending_auto" not in st.session_state:
    st.session_state.code_pending_auto = False

# ---- EARLY ROUTING GUARD: consume pending before any UI can rerun ----
early_prompt = None
if st.session_state.get("pending_prompt") and st.session_state.get("mode") in {"rag", "trip", "code", "general"}:
    # æŠ¢å…ˆæ¶ˆè´¹ pending_promptï¼Œé¿å…è¢«é¦–å¯æç¤º/æŒ‰é’®å†æ¬¡æ‰“æ–­
    early_prompt = st.session_state.pending_prompt
    st.session_state.pending_prompt = None
    # æœ‰ pending æ—¶ï¼Œè·³è¿‡é¦–å¯å¼•å¯¼ï¼Œç›´æ¥è¿›å…¥å¯¹åº”åˆ†æ”¯
    # BUT: Don't skip first_activation if pending is __MODE_ACTIVATED__ (mode just activated)
    if ("first_activation" in st.session_state and
        st.session_state.mode in st.session_state.first_activation and
        early_prompt != "__MODE_ACTIVATED__"):  # âœ… ä¸è·³è¿‡æ¨¡å¼æ¿€æ´»æ—¶çš„é¦–å¯
        st.session_state.first_activation[st.session_state.mode] = False
    debug_log(f"[guard] consumed pending for mode={st.session_state.mode} prompt={early_prompt!r}")
else:
    debug_log(f"[guard] no pending to consume (mode={st.session_state.get('mode')}, pending={st.session_state.get('pending_prompt')!r})")

# st.sidebar.caption(
#     f"DEBUG â€¢ mode={st.session_state.mode} "
#     f"pending={st.session_state.pending_prompt!r} "
#     f"await_confirm={st.session_state.awaiting_confirmation}"
# )
if "code_force_language" not in st.session_state:
    st.session_state.code_force_language: Optional[str] = None

rag_server_config = get_rag_server_config()
rag_limits = rag_server_config.get("limits", {})
vector_min = int(rag_limits.get("vector_min", 6))
vector_max = int(rag_limits.get("vector_max", 20))
content_min = int(rag_limits.get("content_char_min", 150))
content_max = int(rag_limits.get("content_char_max", 1000))
content_default = int(rag_limits.get("content_char_default", 300))
reranker_options = rag_server_config.get("reranker_options", ["auto"])

if "rag_vector_limit" not in st.session_state:
    st.session_state.rag_vector_limit = vector_min
else:
    st.session_state.rag_vector_limit = int(max(vector_min, min(vector_max, st.session_state.rag_vector_limit)))

if "rag_content_limit" not in st.session_state:
    st.session_state.rag_content_limit = content_default
else:
    st.session_state.rag_content_limit = int(max(content_min, min(content_max, st.session_state.rag_content_limit)))

if "rag_reranker_choice" not in st.session_state:
    # Default to fallback (MiniLM) for CPU performance
    st.session_state.rag_reranker_choice = "fallback" if "fallback" in reranker_options else (reranker_options[0] if reranker_options else "auto")
elif st.session_state.rag_reranker_choice not in reranker_options:
    st.session_state.rag_reranker_choice = "fallback" if "fallback" in reranker_options else (reranker_options[0] if reranker_options else "auto")


# =====================================================================
# ä¾§è¾¹æ  - æœåŠ¡çŠ¶æ€å’Œè¯„ä¼°ä»ªè¡¨æ¿
# =====================================================================
with st.sidebar:
    st.title("ğŸ›ï¸ Dashboard")

    st.markdown("### ğŸ“Š Service Status")

    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    try:
        rag_health = check_service_health(f"{BACKEND_URL}/api/rag/health")
    except:
        rag_health = False

    try:
        agent_health = check_service_health(f"{BACKEND_URL}/api/agent/health")
    except:
        agent_health = False

    try:
        code_health = check_service_health(f"{BACKEND_URL}/api/code/health")
    except:
        code_health = False

    # æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
    rag_status = "ğŸŸ¢" if rag_health else "ğŸ”´"
    agent_status = "ğŸŸ¢" if agent_health else "ğŸ”´"
    code_status = "ğŸŸ¢" if code_health else "ğŸ”´"

    rag_active = "âœ…" if st.session_state.mode == "rag" else ""
    agent_active = "âœ…" if st.session_state.mode == "trip" else ""
    code_active = "âœ…" if st.session_state.mode == "code" else ""

    st.markdown(f"{rag_status} **RAG Q&A** {rag_active}")
    st.markdown(f"{agent_status} **Trip Planning** {agent_active}")
    st.markdown(f"{code_status} **Code Generation** {code_active}")

    st.markdown("---")

    st.markdown("### ğŸ“ˆ Evaluation Dashboard")

    agent_metrics_data: Optional[Dict[str, Any]] = None
    try:
        metrics_resp = requests.get(f"{BACKEND_URL}/api/agent/metrics", timeout=2)
        if metrics_resp.status_code == 200:
            agent_metrics_data = metrics_resp.json()
    except Exception:
        agent_metrics_data = None

    # åŸºç¡€ç»Ÿè®¡
    num_messages = len(st.session_state.messages)
    col1, col2 = st.columns(2)
    col1.metric("Messages", num_messages)

    if st.session_state.mode:
        col2.metric("Mode", st.session_state.mode.upper())
    else:
        col2.metric("Mode", "None")

    # Latency & Cost è¶‹åŠ¿ - å§‹ç»ˆæ˜¾ç¤º
    st.markdown("**â±ï¸ Latency Over Time**")
    if len(st.session_state.metrics_history["latencies"]) > 0:
        df = pd.DataFrame({
            "Request": range(1, len(st.session_state.metrics_history["latencies"]) + 1),
            "Latency (ms)": st.session_state.metrics_history["latencies"]
        })
        st.line_chart(df.set_index("Request"))

        # æ˜¾ç¤ºå¹³å‡å€¼å’Œä¸­ä½æ•°
        latencies = st.session_state.metrics_history["latencies"]
        avg_lat = np.mean(latencies)
        median_lat = np.median(latencies)
        col_a, col_b = st.columns(2)
        col_a.metric("Avg", f"{avg_lat:.0f}ms")
        col_b.metric("Median", f"{median_lat:.0f}ms")
    else:
        st.info("ğŸ“Š No data yet - waiting for first request")

    # Cost è¶‹åŠ¿ - å§‹ç»ˆæ˜¾ç¤º
    st.markdown("**ğŸ’° Token Cost Tracking**")
    if len(st.session_state.metrics_history["costs"]) > 0:
        total_cost = sum(st.session_state.metrics_history["costs"])
        st.metric("Total Token Cost", f"${total_cost:.4f}")

        df_cost = pd.DataFrame({
            "Request": range(1, len(st.session_state.metrics_history["costs"]) + 1),
            "Token Cost ($)": st.session_state.metrics_history["costs"]
        })
        st.line_chart(df_cost.set_index("Request"))
    else:
        col_c1, col_c2 = st.columns(2)
        col_c1.metric("Total Token Cost", "$0.0000")
        col_c2.metric("Requests", "0")

    # RAG æ€§èƒ½æŒ‡æ ‡
    if len(st.session_state.rag_metrics["retrieval_times"]) > 0:
        st.markdown("**ğŸ“š RAG Performance**")

        retrieval_times = st.session_state.rag_metrics["retrieval_times"]
        confidences = st.session_state.rag_metrics["confidences"]

        # æ£€ç´¢æ—¶é—´ç»Ÿè®¡
        if retrieval_times:
            avg_retrieval = np.mean(retrieval_times)
            median_retrieval = np.median(retrieval_times)
            col_r1, col_r2 = st.columns(2)
            col_r1.metric("Avg Retrieval", f"{avg_retrieval:.1f}ms")
            col_r2.metric("Median", f"{median_retrieval:.1f}ms")

        # å‡†ç¡®ç‡/ç½®ä¿¡åº¦è¶‹åŠ¿
        if confidences:
            st.markdown("**Confidence Over Time**")
            df_conf = pd.DataFrame({
                "Query": range(1, len(confidences) + 1),
                "Confidence": confidences
            })
            st.line_chart(df_conf.set_index("Query"))

            avg_conf = np.mean(confidences)
            st.metric("Avg Confidence", f"{avg_conf:.3f}")
    st.markdown("---")

    # RAG Q&A Stats
    st.markdown("**ğŸ“š RAG Q&A Stats**")
    rag_stats = st.session_state.rag_stats
    total_rag_queries = rag_stats["total_queries"]

    if total_rag_queries > 0:
        # Use smaller font for sidebar metrics
        st.markdown("""
        <style>
        section[data-testid="stSidebar"] [data-testid="stMetricValue"] {
            font-size: 18px;
        }
        </style>
        """, unsafe_allow_html=True)

        col_rag1, col_rag2, col_rag3, col_rag4 = st.columns(4)
        col_rag1.metric("Total Queries", total_rag_queries)
        col_rag2.metric("Avg Confidence", f"{rag_stats['avg_confidence']:.3f}")
        col_rag3.metric("Primary Uses", rag_stats["primary_reranker_count"])
        col_rag4.metric("Fallback Uses", rag_stats["fallback_reranker_count"])

        # Average latencies
        avg_retrieval = rag_stats["total_retrieval_ms"] / total_rag_queries
        avg_rerank = rag_stats["total_rerank_ms"] / total_rag_queries
        avg_llm = rag_stats["total_llm_ms"] / total_rag_queries
        avg_total = (rag_stats["total_retrieval_ms"] + rag_stats["total_llm_ms"]) / total_rag_queries

        st.markdown("**Average Latency**")
        col_lat1, col_lat2, col_lat3, col_lat4 = st.columns(4)
        col_lat1.metric("Retrieval", f"{avg_retrieval:.1f}ms")
        col_lat2.metric("Rerank", f"{avg_rerank:.1f}ms")
        col_lat3.metric("LLM", f"{avg_llm:.1f}ms")
        col_lat4.metric("Total", f"{avg_total:.1f}ms")

        # Token usage and cost
        avg_tokens = rag_stats["total_tokens"] / total_rag_queries
        total_cost = rag_stats["total_cost_usd"]

        col_tok1, col_tok2 = st.columns(2)
        col_tok1.metric("Avg Tokens/Query", f"{avg_tokens:.0f}")
        col_tok2.metric("Total Cost (USD)", f"${total_cost:.4f}")
    else:
        st.caption("No RAG queries yet")

    st.markdown("---")
    # Agent æˆåŠŸç‡ - å§‹ç»ˆæ˜¾ç¤º
    st.markdown("**âœˆï¸ Trip Agent Stats**")
    agent_stats = st.session_state.agent_stats
    total_attempts = agent_stats["success"] + agent_stats["failure"] + agent_stats["partial"]

    if agent_metrics_data:
        st.caption("Aggregated across all sessions")
        col_s1, col_s2, col_s3 = st.columns(3)
        col_s1.metric("âœ…", agent_metrics_data.get("success_plans", 0))
        col_s2.metric("âš ï¸", agent_metrics_data.get("partial_plans", 0))
        col_s3.metric("âŒ", agent_metrics_data.get("failed_plans", 0))

        col_rate, col_cost = st.columns(2)
        col_rate.metric("Success Rate", f"{agent_metrics_data.get('success_rate', 0.0):.1f}%")
        col_cost.metric("Token Cost (USD)", f"${agent_metrics_data.get('total_cost_usd', 0.0):.4f}")

        col_avg1, col_avg2 = st.columns(2)
        col_avg1.metric("Avg Planning Time", f"{agent_metrics_data.get('avg_planning_time_ms', 0.0):.0f}ms")
        col_avg2.metric("Avg Tool Calls", f"{agent_metrics_data.get('avg_tool_calls_per_plan', 0.0):.1f}")

        history_entries = agent_metrics_data.get("history") or []
        if history_entries:
            st.markdown("**Recent Runs**")
            recent_entries = history_entries[-3:]
            for entry in reversed(recent_entries):
                timestamp_str = entry.get("timestamp")
                display_time = timestamp_str
                if timestamp_str:
                    try:
                        display_time = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00")).strftime("%H:%M:%S")
                    except Exception:
                        display_time = timestamp_str
                outcome = entry.get("outcome", "unknown").title()
                planning_ms = entry.get("planning_time_ms", 0.0)
                cost_usd = entry.get("token_cost_usd", entry.get("cost_usd", 0.0))
                st.caption(f"{display_time} â€¢ {outcome} â€¢ {planning_ms:.0f}ms â€¢ Token Cost ${cost_usd:.4f}")

        if total_attempts > 0:
            st.caption(
                f"This session: {agent_stats['success']} success / "
                f"{agent_stats['partial']} partial / {agent_stats['failure']} failure"
            )
    else:
        col_s1, col_s2, col_s3 = st.columns(3)
        col_s1.metric("âœ…", agent_stats["success"])
        col_s2.metric("âš ï¸", agent_stats["partial"])
        col_s3.metric("âŒ", agent_stats["failure"])

        if total_attempts > 0:
            success_rate = (agent_stats["success"] / total_attempts) * 100
            st.metric("Success Rate", f"{success_rate:.1f}%")
        else:
            st.metric("Success Rate", "N/A")
    st.markdown("---")
    st.markdown("**ğŸ’» Code Agent Stats**")
    code_stats = st.session_state.code_stats
    total_runs_code = code_stats["total_runs"]
    col_code1, col_code2, col_code3, col_code4 = st.columns(4)
    col_code1.metric("Runs", total_runs_code)
    col_code2.metric("Passes", code_stats["passes"])
    col_code3.metric("Failed Tests", code_stats["failures"])
    col_code4.metric("Errors", code_stats["errors"])

    if total_runs_code > 0:
        avg_latency = code_stats["total_latency_ms"] / total_runs_code
        avg_tokens = code_stats["total_tokens"] / total_runs_code
        avg_cost = code_stats["total_cost_usd"] / total_runs_code
    else:
        avg_latency = avg_tokens = avg_cost = 0.0

    col_code_avg1, col_code_avg2, col_code_avg3 = st.columns(3)
    col_code_avg1.metric("Avg Latency", f"{avg_latency:.0f}ms")
    col_code_avg2.metric("Avg Tokens", f"{avg_tokens:.0f}")
    col_code_avg3.metric("Avg Token Cost", f"${avg_cost:.4f}")

    code_history = code_stats.get("history", [])
    if code_history:
        st.markdown("**Recent Code Runs**")
        for entry in reversed(code_history[-3:]):
            ts = entry.get("timestamp")
            if isinstance(ts, datetime):
                display_time = ts.strftime("%H:%M:%S")
            elif isinstance(ts, str):
                display_time = ts
            else:
                display_time = "â€”"
            status = entry.get("status", "unknown").replace("_", " ").title()
            parts = [display_time, status]
            language = entry.get("language")
            if language:
                parts.append(language)
            latency_ms_entry = entry.get("latency_ms")
            if latency_ms_entry is not None:
                parts.append(f"{latency_ms_entry:.0f}ms")
            tokens_entry = entry.get("tokens")
            if tokens_entry is not None:
                parts.append(f"{tokens_entry} tok")
            cost_entry = entry.get("cost_usd")
            if cost_entry is not None:
                parts.append(f"${cost_entry:.4f}")
            exit_code_entry = entry.get("exit_code")
            if exit_code_entry is not None:
                parts.append(f"exit {exit_code_entry}")
            st.caption(" â€¢ ".join(parts))
            message = entry.get("message")
            if message:
                st.caption(f"Message: {message}")
    else:
        st.info("No code runs yet this session")

    st.markdown("---")
    st.text(f"Session: {st.session_state.session_id}")

    if st.button("ğŸ”„ Reset Session"):
        st.session_state.messages = []
        st.session_state.mode = "general"  # Reset to general mode
        st.session_state.trip_constraints = TripConstraints()
        st.session_state.awaiting_confirmation = False
        st.session_state.trip_last_plan = None
        st.session_state.code_stats = {
            "total_runs": 0,
            "passes": 0,
            "failures": 0,
            "errors": 0,
            "total_latency_ms": 0.0,
            "total_tokens": 0,
            "total_cost_usd": 0.0,
            "history": [],
        }
        st.session_state.code_show_samples = True
        st.session_state.code_show_samples_prev = True
        st.session_state.code_last_request = None
        st.session_state.code_pending_auto = False
        st.session_state.code_force_language = None
        # Reset metrics
        st.session_state.metrics_history = {
            "timestamps": [], "latencies": [], "prompt_tokens": [],
            "completion_tokens": [], "costs": [], "services": []
        }
        st.session_state.rag_metrics = {
            "embed_times": [], "rerank_times": [],
            "retrieval_times": [], "confidences": []
        }
        st.session_state.agent_stats = {"success": 0, "failure": 0, "partial": 0}
        try:
            requests.post(f"{BACKEND_URL}/api/agent/metrics/reset", timeout=3)
        except Exception:
            pass
        st.rerun()


# =====================================================================
# ä¸»ç•Œé¢
# =====================================================================

st.title("ğŸ¤– AI Assistant - Complete CLI Replication")

st.markdown("""
Welcome! I'm your AI assistant providing three powerful services:
- ğŸ“š **RAG Q&A**: Ask questions about documents
- âœˆï¸ **Trip Planning**: Plan your perfect trip with intelligent constraint collection
- ğŸ’» **Code Generation**: Generate self-healing code with tests

Use the quick buttons below to activate a service, or just type naturally and I'll understand your intent!
""")

# =====================================================================
# å¿«æ·æŒ‰é’®ï¼ˆ3ä¸ªï¼šRAG, Trip, Codeï¼‰
# =====================================================================

col1, col2, col3 = st.columns(3)

with col1:
    if st.button("ğŸ“š RAG Q&A", use_container_width=True):
        st.session_state.mode = "rag"
        # Set pending_prompt to special marker to activate EARLY ROUTING GUARD
        st.session_state.pending_prompt = "__MODE_ACTIVATED__"
        if st.session_state.first_activation["rag"]:
            append_chat_history(
                "assistant",
                """ğŸ“š **RAG Q&A Mode Activated**

I'll help you search and answer questions from the document collection.

**Examples:**
- "Who wrote DADDY TAKE ME SKATING?"
- "Tell me about American frontier history"
- 'Sir roberts fortune a novel', for what purpose he was confident of his own powers of cheating the uncle, and managing?"

-  Type 'q'(quit) to exit this mode.
""",
            )
            st.session_state.first_activation["rag"] = False
        st.rerun()

with col2:
    if st.button("âœˆï¸ Trip Planning", use_container_width=True):
        st.session_state.mode = "trip"
        # Set pending_prompt to special marker to activate EARLY ROUTING GUARD
        # This prevents the mode from being reset during rerun
        st.session_state.pending_prompt = "__MODE_ACTIVATED__"
        if st.session_state.first_activation["trip"]:
            append_chat_history(
                "assistant",
                """âœˆï¸ **Trip Planning Mode Activated**

I'll help you plan your perfect trip! I need to collect four key pieces of information:
- ğŸ“ Where do you want to go?
- ğŸ›« Where are you leaving from?
- ğŸ“… How many days?
- ğŸ’° What's your budget?

**Examples:**
- "I want to go to Tokyo from Auckland for 5 days with $2000"
- "Plan a trip to Paris, 1 week, budget 3000 NZD, from Wellington"

Type 'q'(quit) to exit this mode.
""",
            )
            st.session_state.first_activation["trip"] = False
        st.rerun()

with col3:
    if st.button("ğŸ’» Code Generation", use_container_width=True):
        st.session_state.mode = "code"
        # Set pending_prompt to special marker to activate EARLY ROUTING GUARD
        st.session_state.pending_prompt = "__MODE_ACTIVATED__"
        if st.session_state.first_activation["code"]:
            append_chat_history(
                "assistant",
                """ğŸ’» **Code Generation Mode Activated**

I'll generate code with automated tests and self-healing capabilities!
- Type 'q'(quit) to exit this mode. 
* Examples:
- 1 "Write a function to check if a number is prime"
- 2 "Create a binary search algorithm in Python"
- 3 "Implement a quick sort in JavaScript"
- 4 "Classic Problem: â€œ

* Most Frequent Character:

* Implement the function:

- def most_frequent_char(s: str) -> str:
    
* Return the character that appears most frequently in the string s.
    If there are multiple characters with the same highest frequency,
    return the one that comes first in alphabetical order.
    

- Example Input and Output (Expected Result):
- print(most_frequent_char("abracadabra"))
- Expected Output: a



""",
            )
            st.session_state.first_activation["code"] = False
        st.rerun()

st.markdown("---")

# =====================================================================
# æ˜¾ç¤ºèŠå¤©å†å²
# =====================================================================

for message in st.session_state.messages:
    content = message.get("content", "")
    if message.get("role") == "assistant":
        content = clean_text_lines(content)
        if not content:
            continue
    with st.chat_message(message["role"]):
        st.markdown(content)

# Auto-scroll to bottom of chat after new messages
st.markdown(
    """
    <script>
        var chatContainer = window.parent.document.querySelector('section.main');
        if (chatContainer) {
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }
    </script>
    """,
    unsafe_allow_html=True
)

last_trip_plan = st.session_state.get("trip_last_plan")
if last_trip_plan and st.session_state.mode != "trip":
    with st.expander("ğŸ§³ Last Trip Plan", expanded=False):
        render_trip_plan_summary(last_trip_plan)


# =====================================================================
# èŠå¤©è¾“å…¥
# =====================================================================

# Code mode options - show before chat input when in code mode
if st.session_state.mode == "code":
    st.markdown("### âš™ï¸ Code Generation Options")

    prev_toggle = st.session_state.code_show_samples_prev
    samples_toggle = st.checkbox(
        "Show assertion outputs (slower)",
        value=st.session_state.code_show_samples,
        key="code_samples_checkbox_main",
        help="When enabled, print statements will be injected before assertions to show actual values"
    )
    st.session_state.code_show_samples = samples_toggle
    toggle_changed = samples_toggle != prev_toggle
    st.session_state.code_show_samples_prev = samples_toggle

    # Show re-run button only if there's a last successful request
    if st.session_state.get('code_last_request'):
        st.caption(f"ğŸ“ Last: {st.session_state.code_last_request.get('prompt', '')[:50]}...")
        if st.button("â™»ï¸ Re-run Last Request", key="code_rerun_button", use_container_width=True):
            st.session_state.pending_prompt = st.session_state.code_last_request.get("prompt")
            st.session_state.code_force_language = st.session_state.code_last_request.get("language")
            st.session_state.code_pending_auto = True
            st.rerun()
    else:
        st.caption("â„¹ï¸ Submit a code request first")

    # If toggled and we have a last successful request, auto re-run
    if toggle_changed and st.session_state.code_last_request:
        st.info("â™»ï¸ Re-running last request with new settings...")
        st.session_state.pending_prompt = st.session_state.code_last_request.get("prompt")
        st.session_state.code_force_language = st.session_state.code_last_request.get("language")
        st.session_state.code_pending_auto = True
        st.rerun()

    st.divider()

# Chat input - always display it
prompt = early_prompt
user_input = st.chat_input("Type your message...")

# DEBUG: Show input status on page
# if user_input:
    # st.sidebar.warning(f"ğŸ” DEBUG: user_input = {user_input!r}")
# st.sidebar.info(f"ğŸ” DEBUG: early_prompt = {early_prompt!r}, pending = {st.session_state.pending_prompt!r}")

# å¦‚æœ early_prompt å·²ç»æä¾›äº† promptï¼Œç›´æ¥ä½¿ç”¨
if prompt:
    debug_log(f"[early] using prompt from guard={prompt!r}")
# å¦åˆ™æ£€æŸ¥ pending_prompt
elif st.session_state.pending_prompt:
    prompt = st.session_state.pending_prompt
    debug_log(f"[pending] reuse prompt={prompt!r}")
    st.session_state.pending_prompt = None  # Clear it
# æœ€åå°è¯•ä½¿ç”¨ç”¨æˆ·è¾“å…¥
elif user_input:
    prompt = user_input
    print(f"ğŸ“¥ USER INPUT RECEIVED: {prompt!r}")
    debug_log(f"[input] new prompt={prompt!r}")
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    append_chat_history("user", prompt)
    with st.chat_message("user"):
        st.markdown(prompt)
    print(f"âœ… User message displayed")

    detected_mode = detect_mode_from_prompt(prompt)
    if detected_mode and detected_mode not in {None, st.session_state.mode}:
        st.session_state.mode = detected_mode
        st.session_state.pending_prompt = prompt
        debug_log(f"[intent] detected={detected_mode} pending set")
        st.rerun()
else:
    prompt = None
    debug_log("[pending] no prompt available")

current_mode = st.session_state.mode
# st.sidebar.write(f"DEBUG â€¢ branch_gate: mode={current_mode}, prompt_truthy={bool(prompt)}, prompt={repr(prompt)[:80]}")

print(f"ğŸ¯ MODE={current_mode!r} PROMPT={prompt!r}")
debug_log(f"[mode] after prompt selection mode={current_mode} prompt={prompt!r}")
debug_log(
    f"[check] trip_condition={bool(st.session_state.mode == 'trip' and prompt)} "
    f"mode={current_mode} prompt_truthy={bool(prompt)}"
)

if current_mode == "rag":
    if prompt and _is_quit(prompt):
        st.session_state.mode = "general"
        st.session_state.awaiting_confirmation = False
        append_chat_history("assistant", "ğŸ‘‹ å·²é€€å‡º RAG æ¨¡å¼ã€‚")
        st.rerun()

    debug_log(f"[rag] branch start mode={current_mode} prompt={prompt!r}")
    st.markdown("---")
    rag_controls_container = st.container()
    render_rag_controls(rag_controls_container)
    st.markdown("---")

    # Warm-up RAG models on first access
    if "rag_warmed_up" not in st.session_state:
        st.session_state.rag_warmed_up = False

    if not st.session_state.rag_warmed_up:
        seed_status_box = st.empty()
        seed_ready = False
        response_content = "ğŸ‘‹ Qdrant warm-up skipped. You can still ask a question once ready."

        for _ in range(180):
            status = fetch_seed_status()
            if status:
                state = status.get("state") or "idle"
                seeded = int(status.get("seeded") or 0)
                total = int(status.get("total") or 0)
                message = status.get("message") or ""
                pct = min(int(seeded / total * 100), 100) if total else 0

                if state in {"idle", "checking", "initializing", "counting"}:
                    seed_status_box.info(message or "Preparing Qdrant collectionâ€¦")
                elif state == "in_progress":
                    seed_status_box.info(f"Seeding Qdrantâ€¦ {seeded}/{total} ({pct}%)")
                elif state == "completed":
                    seed_status_box.success(f"Qdrant ready ({seeded}/{total})")
                    seed_ready = True
                    break
                elif state == "error":
                    seed_status_box.warning(f"Qdrant seeding error: {message}")
                    break
                else:
                    seed_status_box.info(message or "Waiting for Qdrant seed statusâ€¦")
            else:
                seed_status_box.info("Waiting for Qdrant seed statusâ€¦")
            time.sleep(1)
        else:
            seed_status_box.warning("Timed out waiting for Qdrant seed completion.")

        backend_ready = wait_for_backend_ready() if seed_ready else False

        if seed_ready and backend_ready:
            with st.spinner("ğŸ”¥ Warming up RAG models (15 queries from eval data)..."):
                try:
                    # Load real eval questions for warm-up (3 from each of 3 eval files = 9 total)
                    # This ensures warm-up queries are similar to actual usage
                    # Need 15 queries to fully warm up: Embedding JIT, Qdrant cache, Reranker session
                    # Use fallback (MiniLM) for CPU performance
                    warmup_questions = load_warmup_questions()

                    for i, question in enumerate(warmup_questions, 1):
                        requests.post(
                            f"{BACKEND_URL}/api/rag/ask",
                            json={
                                "question": question,
                                "top_k": 5,
                                "include_timings": True,  # Use same code path as real queries
                                "reranker": "fallback",
                                "vector_limit": 5,
                                "content_char_limit": 300
                            },
                            timeout=30
                        )
                        # Show progress every 5 queries
                        if i % 5 == 0:
                            st.toast(f"ğŸ”¥ Warming up... {i}/15 queries", icon="ğŸ”¥")

                    st.session_state.rag_warmed_up = True
                    st.success("âœ… Models fully warmed up! Ready for fast queries.")
                    time.sleep(1)
                    response_content = "âœ… Qdrant and models are ready! Ask me something about the documents."
                except Exception as e:
                    st.warning(f"âš ï¸ Warmup failed: {e}")
                    response_content = f"âš ï¸ Warm-up failed: {e}"
        else:
            if not seed_ready:
                st.warning("âš ï¸ Qdrant seeding not completed, skipping warm-up this run.")
                response_content = "âš ï¸ Qdrant seeding not completed; try again shortly or run a manual test."
            elif not backend_ready:
                st.warning("âš ï¸ Backend not ready, skipping warm-up this run.")
                response_content = "âš ï¸ Backend not ready yet. Please retry once it's healthy."

        with st.chat_message("assistant"):
            st.markdown(response_content)
        # Keep RAG mode active after warm-up instead of resetting to None
        # st.session_state.mode = None  # BUG: This resets mode to general AI
        st.session_state.awaiting_confirmation = False
        # st.stop() - removed to allow page to render

    # =====================================================================
    # RAG Mode - å®Œæ•´å¤åˆ» chat_rag.py
    # =====================================================================

    if current_mode == "rag" and prompt and prompt != "__MODE_ACTIVATED__":  # Only process if we have a real prompt
        with st.chat_message("assistant"):
            with st.spinner("ğŸ” Searching documents..."):
                try:
                    payload = {
                        "question": prompt,
                        "top_k": 5,
                        "include_timings": True,
                    }

                    reranker_choice = st.session_state.get("rag_reranker_choice")
                    if reranker_choice:
                        payload["reranker"] = reranker_choice

                    vector_limit_payload = int(max(vector_min, min(vector_max, st.session_state.get("rag_vector_limit", vector_min))))
                    payload["vector_limit"] = vector_limit_payload

                    content_limit_payload = int(max(content_min, min(content_max, st.session_state.get("rag_content_limit", content_default))))
                    payload["content_char_limit"] = content_limit_payload

                    response = requests.post(
                        f"{BACKEND_URL}/api/rag/ask",
                        json=payload,
                        timeout=30
                    )

                    if response.status_code == 200:
                        result = response.json()

                        # Check if query was slow and auto-switch to fallback
                        total_ms = result.get("total_time_ms", result.get("timings", {}).get("end_to_end_ms", 0.0))
                        current_reranker = st.session_state.get("rag_reranker_choice", "primary")

                        # Auto-switch to fallback if primary took > 300ms and not already on fallback
                        if total_ms > 300 and current_reranker == "primary" and "fallback" in reranker_options:
                            st.session_state.rag_reranker_choice = "fallback"
                            st.session_state.rag_last_reranker = "fallback"
                            st.warning(f"âš¡ Query took {total_ms:.1f}ms (>300ms). Auto-switched to Fallback (MiniLM) for faster responses.")

                            # Warm up fallback with 15 queries from eval data
                            with st.spinner("ğŸ”¥ Warming up Fallback reranker (15 queries)..."):
                                try:
                                    # Load real eval questions for warm-up
                                    warmup_questions = load_warmup_questions()

                                    for i, question in enumerate(warmup_questions, 1):
                                        warmup_response = requests.post(
                                            f"{BACKEND_URL}/api/rag/ask",
                                            json={
                                                "question": question,
                                                "top_k": 3,
                                                "include_timings": True,  # Use same code path as real queries
                                                "reranker": "fallback",
                                                "vector_limit": 5,
                                                "content_char_limit": 300
                                            },
                                            timeout=30
                                        )
                                    st.success("âœ… Fallback reranker ready!")
                                except Exception as e:
                                    st.warning(f"âš ï¸ Fallback warm-up failed: {e}")

                        # æ˜¾ç¤ºç­”æ¡ˆ
                        answer = result.get("answer", "")
                        st.markdown(f"**Answer:**\n\n{answer}")

                        # æ˜¾ç¤ºæŒ‡æ ‡
                        st.markdown("---")
                        col1, col2, col3 = st.columns(3)
                        col1.metric("Retrieval Time", f"{result.get('retrieval_time_ms', 0):.1f}ms")
                        col2.metric("Confidence", f"{result.get('confidence', 0):.3f}")
                        col3.metric("Chunks", result.get('num_chunks_retrieved', 0))

                        timings = result.get("timings") or {}
                        models_info = result.get("models") or {}
                        llm_ms = timings.get("llm_ms", result.get("llm_time_ms", 0.0))
                        token_usage = result.get("token_usage") or {}
                        token_cost_usd = float(result.get("token_cost_usd") or 0.0)
                        prompt_tokens = int(token_usage.get("prompt", 0) or 0)
                        completion_tokens = int(token_usage.get("completion", 0) or 0)
                        total_tokens = int(token_usage.get("total", prompt_tokens + completion_tokens) or 0)

                        if timings:
                            st.markdown("**Latency Breakdown**")
                            breakdown_cols = st.columns(5)
                            breakdown_cols[0].metric("Embed", f"{timings.get('embed_ms', 0.0):.1f}ms")
                            breakdown_cols[1].metric("Vector", f"{timings.get('vector_ms', 0.0):.1f}ms")
                            breakdown_cols[2].metric("Rerank", f"{timings.get('rerank_ms', 0.0):.1f}ms")
                            breakdown_cols[3].metric("LLM", f"{llm_ms:.1f}ms")
                            breakdown_cols[4].metric("Total", f"{total_ms:.1f}ms")

                        if token_usage or token_cost_usd:
                            st.markdown("**Token Usage & Cost**")
                            token_cols = st.columns(4)
                            token_cols[0].metric("Prompt Tokens", prompt_tokens)
                            token_cols[1].metric("Completion Tokens", completion_tokens)
                            token_cols[2].metric("Total Tokens", total_tokens)
                            token_cols[3].metric("Token Cost (USD)", f"${token_cost_usd:.4f}")

                        vector_used = result.get("vector_limit_used") or timings.get("vector_limit_used")
                        content_used = result.get("content_char_limit_used") or timings.get("content_char_limit_used")
                        reranker_mode = result.get("reranker_mode") or timings.get("reranker_mode")
                        capsule = []
                        if vector_used is not None:
                            capsule.append(f"vector limit {vector_used}")
                        if content_used:
                            capsule.append(f"content limit {content_used}")
                        else:
                            capsule.append("full content")
                        if reranker_mode:
                            capsule.append(f"reranker mode {reranker_mode}")
                        st.caption(" | ".join(capsule))

                        if models_info:
                            st.markdown("**Models Used**")
                            st.markdown(
                                f"- Embedding: `{format_model_label(models_info.get('embedding'))}`\n"
                                f"- Reranker: `{format_model_label(models_info.get('reranker'))}`\n"
                                f"- LLM: `{models_info.get('llm', 'â€”')}`"
                            )

                        # Update RAG stats
                        retrieval_ms = result.get('retrieval_time_ms', 0.0)
                        rerank_ms = timings.get('rerank_ms', 0.0)
                        confidence = result.get('confidence', 0.0)
                        current_reranker_choice = st.session_state.get("rag_reranker_choice", "primary")

                        st.session_state.rag_stats["total_queries"] += 1
                        st.session_state.rag_stats["total_retrieval_ms"] += retrieval_ms
                        st.session_state.rag_stats["total_rerank_ms"] += rerank_ms
                        st.session_state.rag_stats["total_llm_ms"] += llm_ms
                        st.session_state.rag_stats["total_tokens"] += total_tokens
                        st.session_state.rag_stats["total_cost_usd"] += token_cost_usd

                        # Update average confidence
                        old_avg = st.session_state.rag_stats["avg_confidence"]
                        n = st.session_state.rag_stats["total_queries"]
                        st.session_state.rag_stats["avg_confidence"] = ((old_avg * (n - 1)) + confidence) / n

                        # Track reranker usage
                        if current_reranker_choice == "primary":
                            st.session_state.rag_stats["primary_reranker_count"] += 1
                        elif current_reranker_choice == "fallback":
                            st.session_state.rag_stats["fallback_reranker_count"] += 1

                        # æ˜¾ç¤ºå¼•ç”¨æ¥æº
                        citations = result.get("citations", [])
                        if citations:
                            with st.expander("ğŸ“ View Sources"):
                                for i, citation in enumerate(citations, 1):
                                    st.markdown(f"**[{i}] {citation.get('source', 'Unknown')}**")
                                    st.markdown(f"- Score: {citation.get('score', 0):.3f}")
                                    content = citation.get('content', '')
                                    snippet = content[:200] + "..." if len(content) > 200 else content
                                    st.markdown(f"- Content: {snippet}")
                                    st.markdown("")

                        # è¯¢é—®æ˜¯å¦ç»§ç»­
                        st.markdown("\n**Continue asking questions or type 'q'(quit) to exit RAG mode.**")

                        # æ”¶é›†RAGæŒ‡æ ‡
                        st.session_state.rag_metrics["retrieval_times"].append(result.get('retrieval_time_ms', 0))
                        st.session_state.rag_metrics["confidences"].append(result.get('confidence', 0))

                        # ä¿å­˜åˆ°æ¶ˆæ¯å†å²å¹¶ç«‹å³æ˜¾ç¤º
                        metrics_parts = [
                            f"Retrieval: {result.get('retrieval_time_ms', 0):.1f}ms",
                            f"Embed: {timings.get('embed_ms', 0.0):.1f}ms",
                            f"Vector: {timings.get('vector_ms', 0.0):.1f}ms",
                            f"Rerank: {timings.get('rerank_ms', 0.0):.1f}ms",
                            f"LLM: {llm_ms:.1f}ms",
                        ]
                        if prompt_tokens or completion_tokens:
                            metrics_parts.append(f"Tokens: {total_tokens}")
                        if token_cost_usd:
                            metrics_parts.append(f"Token Cost: ${token_cost_usd:.4f}")
                        metrics_summary = " | ".join(metrics_parts)
                        model_summary = (
                            f"Embedding: {format_model_label(models_info.get('embedding'))}, "
                            f"Reranker: {format_model_label(models_info.get('reranker'))}, "
                            f"LLM: {models_info.get('llm', 'â€”')}"
                        )

                        full_response = (
                            f"**Answer:**\n\n{answer}\n\n---\n\n"
                            f"**Metrics:** {metrics_summary} | Confidence: {result.get('confidence', 0):.3f} "
                            f"| Chunks: {result.get('num_chunks_retrieved', 0)}\n"
                            f"**Models:** {model_summary}\n\n"
                            "**Continue asking questions or type 'q'(quit) to exit RAG mode.**"
                        )
                        append_chat_history("assistant", full_response)

                        stored_token_usage = None
                        if token_usage or prompt_tokens or completion_tokens:
                            stored_token_usage = {
                                "prompt": prompt_tokens,
                                "completion": completion_tokens,
                                "total": total_tokens,
                            }

                        st.session_state.metrics_history["timestamps"].append(datetime.now())
                        st.session_state.metrics_history["latencies"].append(total_ms)
                        st.session_state.metrics_history["prompt_tokens"].append(prompt_tokens)
                        st.session_state.metrics_history["completion_tokens"].append(completion_tokens)
                        st.session_state.metrics_history["costs"].append(token_cost_usd)
                        st.session_state.metrics_history["services"].append("rag")

                        st.session_state.rag_last_summary = {
                            "timings": {
                                "embed_ms": timings.get("embed_ms", 0.0),
                                "vector_ms": timings.get("vector_ms", 0.0),
                                "rerank_ms": timings.get("rerank_ms", 0.0),
                                "llm_ms": llm_ms,
                                "end_to_end_ms": total_ms,
                            },
                            "models": models_info,
                            "vector_limit": vector_used,
                            "content_limit": content_used,
                            "reranker_mode": reranker_mode,
                            "token_usage": stored_token_usage,
                            "token_cost_usd": token_cost_usd,
                        }

                    else:
                        error_msg = f"âŒ Error: {response.status_code}"
                        st.error(error_msg)
                        append_chat_history("assistant", error_msg)

                except Exception as e:
                    error_msg = f"âŒ Request failed: {e}"
                    st.error(error_msg)
                    append_chat_history("assistant", error_msg)

    # =====================================================================
    # Trip Planning Mode - å®Œæ•´å¤åˆ» chat_agent.py
    # =====================================================================

elif current_mode == "trip" and prompt and prompt != "__MODE_ACTIVATED__":
    if prompt and _is_quit(prompt):
        st.session_state.mode = "general"
        st.session_state.awaiting_confirmation = False
        append_chat_history("assistant", "ğŸ‘‹ å·²é€€å‡º RAG æ¨¡å¼ã€‚")
        st.rerun()


    constraints_state = st.session_state.get("trip_constraints")
    debug_log(f"[trip] constraints_state type={type(constraints_state)}")
    try:
        constraints_dump = constraints_state.model_dump(exclude_none=True)  # type: ignore[attr-defined]
    except AttributeError:
        constraints_dump = str(constraints_state)
    debug_log(
        f"[trip] prompt={prompt!r} awaiting={st.session_state.awaiting_confirmation} "
        f"constraints_type={type(constraints_state)} constraints={constraints_dump}"
    )
    with st.chat_message("assistant"):
        debug_log("[trip] entered assistant chat block")
        # ç‰¹æ®Šå‘½ä»¤ï¼šstatus
        if prompt.lower() in ['status', 'info', 'show']:
            summary = format_constraints_summary(st.session_state.trip_constraints)
            is_complete, missing = check_constraints_complete(st.session_state.trip_constraints)

            st.markdown("ğŸ“ **Current trip information:**")
            st.code(summary)

            if not is_complete:
                st.warning(f"âš ï¸ Missing: {', '.join(missing)}")
            else:
                is_valid, issues = validate_constraints(st.session_state.trip_constraints)
                if not is_valid:
                    st.warning(f"âš ï¸ Issues: {', '.join(issues)}")
                else:
                    st.success("âœ… All information complete and valid!")

            append_chat_history("assistant", "Status displayed")
            # st.stop() - removed to allow page to render

        # commandï¼šreset
        if prompt.lower() in ['reset', 'clear', 'restart']:
            st.session_state.trip_constraints = TripConstraints()
            st.session_state.session_mgr.save_constraints(
                st.session_state.session_id,
                st.session_state.trip_constraints
            )
            st.session_state.trip_last_plan = None
            st.success("ğŸ”„ Trip information cleared. Let's start fresh!")
            append_chat_history("assistant", "Reset complete")
            # st.stop() - removed to allow page to render

        # å¦‚æœæ­£åœ¨ç­‰å¾…ç¡®è®¤
        if st.session_state.awaiting_confirmation:
            user_response = prompt.lower().strip()

            # Handle quit/exit
            if user_response in ['q', 'quit', 'exit', 'cancel']:
                st.info("ğŸ‘‹ Trip planning cancelled. Feel free to start a new trip anytime!")
                append_chat_history("assistant", "Trip planning cancelled")
                st.session_state.awaiting_confirmation = False
                st.session_state.trip_constraints = TripConstraints()  # Reset

            # Handle re-create/restart
            elif user_response in ['r', 'recreate', 're-create', 'restart', 'start over']:
                st.info("ğŸ”„ Let's start over! Please tell me about your trip plans.")
                append_chat_history("assistant", "Restarting trip planning from scratch")
                st.session_state.awaiting_confirmation = False
                st.session_state.trip_constraints = TripConstraints()  # Reset all constraints

            # Handle confirmation to proceed
            elif user_response in ['yes', 'y', 'sure', 'ok', 'okay', 'go', 'proceed']:
                # æ‰§è¡Œè§„åˆ’
                print("[Trip] Confirmation received, calling /api/agent/plan")
                with st.spinner("ğŸ”„ Planning your trip..."):
                    try:
                        # å¡«å……å¼€å§‹æ—¥æœŸ
                        if not st.session_state.trip_constraints.start_date:
                            start = date.today() + timedelta(days=7)
                            st.session_state.trip_constraints.start_date = start.isoformat()
                            if st.session_state.trip_constraints.days:
                                end = start + timedelta(days=st.session_state.trip_constraints.days - 1)
                                st.session_state.trip_constraints.end_date = end.isoformat()

                        payload = {
                            "prompt": f"Plan a {st.session_state.trip_constraints.days}-day trip to {st.session_state.trip_constraints.destination_city}",
                            "constraints": st.session_state.trip_constraints.model_dump(exclude_none=True),
                            "max_iterations": 5,
                        }
                        plan_resp = requests.post(
                            f"{BACKEND_URL}/api/agent/plan",
                            json=payload,
                            timeout=120,
                        )
                        print(
                            f"[Trip] Plan response status={plan_resp.status_code} "
                            f"content_type={plan_resp.headers.get('content-type')}"
                        )
                        if plan_resp.status_code != 200:
                            raise RuntimeError(
                                f"Backend returned {plan_resp.status_code}: {plan_resp.text}"
                            )
                        response = plan_resp.json()
                        st.session_state.trip_last_plan = response

                        # result
                        st.success("âœ… Here's your trip plan!")

                        itinerary = response.get("itinerary", {})
                        destination = itinerary.get("destination", "Unknown")
                        currency = itinerary.get("currency") or infer_currency_from_origin(
                            st.session_state.trip_constraints.origin_city
                        ) or "USD"
                        currency = currency.upper()
                        st.session_state.trip_constraints.currency = currency
                        st.markdown(f"**ğŸ“ Destination:** {destination}")

                        # flight
                        flights = itinerary.get("flights") or []
                        if flights:
                            st.markdown("**âœˆï¸ Flights:**")
                            for i, flight in enumerate(flights[:2], 1):
                                airline = flight.get("airline", "Unknown airline")
                                number = flight.get("flight_number", "N/A")
                                st.markdown(f"{i}. {airline} {number}")
                                st.markdown(f"   - {flight.get('departure_time', '?')} â†’ {flight.get('arrival_time', '?')}")
                                flight_price = float(flight.get("price", 0.0))
                                display_currency = (flight.get("currency") or currency).upper()
                                st.markdown(f"   - Price: {display_currency} {flight_price:.2f}")
                                original_currency = flight.get("original_currency")
                                original_price = flight.get("original_price")
                                if (
                                    original_currency
                                    and original_price is not None
                                    and original_currency.upper() != display_currency
                                ):
                                    st.caption(
                                        f"     (~ {original_currency.upper()} {float(original_price):.2f} before conversion)"
                                    )
                                st.markdown(f"   - Duration: {flight.get('duration_hours', 0)} hours")

                        # weather
                        weather = itinerary.get("weather_forecast") or []
                        if weather:
                            st.markdown("**ğŸŒ¤ï¸ Weather Forecast:**")
                            for day in weather[:3]:
                                st.markdown(
                                    f"   - {day.get('date', '?')}: {day.get('temperature_celsius', 0)}Â°C, {day.get('condition', '')}"
                                )

                        # landmark
                        attractions = itinerary.get("attractions") or []
                        if attractions:
                            st.markdown("**ğŸ¯ Top Attractions:**")
                            for i, attr in enumerate(attractions[:5], 1):
                                st.markdown(
                                    f"{i}. {attr.get('name', 'Attraction')} ({attr.get('category', '')}) - â­ {attr.get('rating', 0)}/5"
                                )
                                st.markdown(f"   - {attr.get('price_range', 'N/A')}")

                        # è´¹ç”¨
                        st.markdown("**ğŸ’° Cost Breakdown:**")
                        cost_breakdown = itinerary.get("cost_breakdown") or {}
                        if cost_breakdown:
                            flight_cost = float(cost_breakdown.get("flights", 0.0))
                            accommodation_cost = float(cost_breakdown.get("accommodation", 0.0))
                            meals_cost = float(cost_breakdown.get("meals", 0.0))
                            other_cost = float(cost_breakdown.get("other", 0.0))

                            if flight_cost > 0:
                                st.markdown(f"- Flights: {currency} {flight_cost:.2f}")
                            if accommodation_cost > 0:
                                st.markdown(f"- Accommodation: {currency} {accommodation_cost:.2f}")
                            if meals_cost > 0:
                                st.markdown(f"- Meals: {currency} {meals_cost:.2f}")
                            if other_cost > 0:
                                st.markdown(f"- Transport & Activities: {currency} {other_cost:.2f}")
                        else:
                            st.caption("Cost breakdown unavailable.")

                        total_cost = itinerary.get("total_cost")
                        total_cost_usd = itinerary.get("total_cost_usd")
                        llm_usage = response.get("llm_token_usage") or {}
                        llm_prompt_tokens = int(llm_usage.get("prompt", 0) or 0)
                        llm_completion_tokens = int(llm_usage.get("completion", 0) or 0)
                        llm_total_tokens = int(llm_usage.get("total", llm_prompt_tokens + llm_completion_tokens) or 0)
                        llm_cost_usd = float(response.get("llm_cost_usd") or 0.0)
                        if total_cost is not None:
                            st.markdown(f"**ğŸ’µ Total: {currency} {total_cost:.2f}**")
                            if total_cost_usd is not None and total_cost_usd > 0:
                                st.caption(f"(â‰ˆ USD {total_cost_usd:.2f})")

                        currency_note = itinerary.get("currency_note")
                        if currency_note:
                            st.caption(currency_note)

                        if llm_total_tokens or llm_cost_usd:
                            st.markdown("**LLM Token Usage**")
                            token_cols = st.columns(4)
                            token_cols[0].metric("Prompt", llm_prompt_tokens)
                            token_cols[1].metric("Completion", llm_completion_tokens)
                            token_cols[2].metric("Total", llm_total_tokens)
                            token_cols[3].metric("Token Cost (USD)", f"${llm_cost_usd:.4f}")

                        fx_rates = itinerary.get("fx_rates") or {}
                        if fx_rates:
                            fx_lines = []
                            for key, value in fx_rates.items():
                                if not isinstance(value, (int, float)):
                                    continue
                                if "->" in key:
                                    src, tgt = key.split("->", 1)
                                    fx_lines.append(f"1 {src.upper()} â‰ˆ {value:.4f} {tgt.upper()}")
                                else:
                                    fx_lines.append(f"1 {key.upper()} â‰ˆ {value:.4f} {currency}")
                            if fx_lines:
                                st.caption("FX rates: " + "; ".join(fx_lines))

                        # çº¦æŸæ»¡è¶³æƒ…å†µ
                        if response.get("constraints_satisfied", False):
                            st.success("âœ… All constraints satisfied!")
                        else:
                            st.warning("âš ï¸ Constraint issues:")
                            for violation in response.get("constraint_violations", []):
                                st.markdown(f"- {violation}")

                        tool_calls = response.get("tool_calls") or []
                        st.markdown(f"\nğŸ”§ Tools used: {len(tool_calls)} calls in {response.get('total_iterations', 0)} iterations")
                        st.markdown(f"â±ï¸ Planning time: {response.get('planning_time_ms', 0):.0f}ms")

                        agent_stats = st.session_state.agent_stats
                        if response.get("constraints_satisfied", False):
                            agent_stats["success"] += 1
                        else:
                            agent_stats["partial"] += 1

                        planning_latency = response.get("planning_time_ms", 0.0)
                        token_cost_usd = llm_cost_usd

                        st.session_state.metrics_history["timestamps"].append(datetime.now())
                        st.session_state.metrics_history["latencies"].append(planning_latency)
                        st.session_state.metrics_history["prompt_tokens"].append(llm_prompt_tokens)
                        st.session_state.metrics_history["completion_tokens"].append(llm_completion_tokens)
                        st.session_state.metrics_history["costs"].append(token_cost_usd)
                        st.session_state.metrics_history["services"].append("trip")

                        st.markdown("\n**Would you like to make any changes to this plan? (or type 'q'(quit) to exit)**")

                        append_chat_history("assistant", "Trip plan created")
                        st.session_state.awaiting_confirmation = False

                    except Exception as e:
                        st.error(f"âŒ Sorry, I encountered an error: {e}")
                        append_chat_history("assistant", f"Error: {e}")
                        st.session_state.agent_stats["failure"] += 1
                        st.session_state.awaiting_confirmation = False

            # Handle modifications/updates to constraints
            else:
                # The user provided modification text instead of y/r/q
                # Process it as a constraint update
                st.session_state.awaiting_confirmation = False
                # Set pending_prompt to re-process this input
                st.session_state.pending_prompt = prompt
                st.info("ğŸ“ Updating your trip information...")
                st.rerun()

        # æ­£å¸¸çº¦æŸæ”¶é›†æµç¨‹
        else:
            # æ€è€ƒæ­¥éª¤ï¼šæå–çº¦æŸ
            print("[Trip] Extracting constraints from prompt")
            debug_log("[trip] extracting constraints")
            with st.spinner("ğŸ¤” Analyzing your request..."):
                old_constraints = st.session_state.trip_constraints.model_copy(deep=True)
                new_constraints, used_llm = extract_constraints_hybrid(
                    prompt,
                    st.session_state.trip_constraints
                )
                st.session_state.trip_constraints = new_constraints
                try:
                    new_dump = new_constraints.model_dump(exclude_none=True)
                except AttributeError:
                    new_dump = str(new_constraints)
                debug_log(
                    f"[trip] extracted constraints={new_dump} used_llm={used_llm}"
                )

                if used_llm:
                    st.info("ğŸ¤– Used LLM to understand city names")

            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ä¿¡æ¯
            has_new_info = (
                new_constraints.destination_city != old_constraints.destination_city or
                new_constraints.origin_city != old_constraints.origin_city or
                new_constraints.days != old_constraints.days or
                new_constraints.budget != old_constraints.budget
            )

            # æ˜¾ç¤ºæå–çš„ä¿¡æ¯
            if has_new_info:
                st.session_state.session_mgr.save_constraints(
                    st.session_state.session_id,
                    new_constraints
                )
                st.success("âœ… Got it! Here's what I understand:")
                st.code(format_constraints_summary(new_constraints))

            # æ£€æŸ¥å®Œæ•´æ€§
            is_complete, missing = check_constraints_complete(new_constraints)

            if not is_complete:
                if not has_new_info:
                    st.info("ğŸ¤” I'd love to help! To plan your trip, I need some information.")

                st.warning(f"âš ï¸ Still need: {', '.join(missing)}")

                if "destination" in missing:
                    st.markdown("**Where would you like to go?**")
                elif "origin" in missing:
                    st.markdown("**Which city will you be departing from?**")
                elif "days" in missing:
                    st.markdown("**How many days will your trip last?**")
                elif "budget" in missing:
                    st.markdown("**What's your total budget?** (e.g., 500 NZD, $1000)")

                append_chat_history("assistant", f"Still need: {', '.join(missing)}")

            else:
                # éªŒè¯åˆç†æ€§
                is_valid, issues = validate_constraints(new_constraints)

                if not is_valid:
                    st.warning("âš ï¸ I noticed some potential issues:")
                    for issue in issues:
                        st.markdown(f"- {issue}")
                    st.markdown("\n**Would you like to proceed anyway, or update the information?**")
                    st.markdown("(Type 'y'(yes) to proceed, or provide updated information)")

                    append_chat_history("assistant", f"Issues found: {', '.join(issues)}")

                else:
                    # ä¿¡æ¯å®Œæ•´ä¸”åˆç†ï¼Œè¯¢é—®ç¡®è®¤
                    st.success("âœ… Perfect! I have all the information:")
                    st.code(format_constraints_summary(new_constraints))
                    st.markdown("\n**Ready to create your trip plan?**")
                    st.markdown("**Options:**\n- 'y' (yes) - Create the trip plan\n- 'r' (re-create) - Start over from scratch\n- 'q' (quit) - Cancel trip planning\n- Or provide updated information to modify")

                    st.session_state.awaiting_confirmation = True
                    append_chat_history("assistant", "Awaiting confirmation")

# =====================================================================
# Code Generation Mode - 
# =====================================================================

elif current_mode == "code" and prompt and prompt != "__MODE_ACTIVATED__":
    print(
        f"[Code] prompt={prompt!r} "
        f"pending_auto={st.session_state.code_pending_auto} "
        f"force_lang={st.session_state.get('code_force_language')}"
    )
    if prompt and _is_quit(prompt):
        st.session_state.mode = "general"
        st.session_state.awaiting_confirmation = False
        append_chat_history("assistant", "ğŸ‘‹ å·²é€€å‡º RAG æ¨¡å¼ã€‚")
        st.rerun()

    with st.chat_message("assistant"):
        # æ£€æµ‹è¯­è¨€
        import re
        detected_lang = "python"  # default

        lang_patterns = {
            "bash": r'\b(bash|shell|sh script|bash script|echo|#!/bin/bash|#!/bin/sh)\b',
            "c": r'\b(in c\b|using c\b|c code|c language|#include|printf|main\(\))\b',
            "cpp": r'\b(c\+\+|cpp|in c\+\+|using c\+\+|std::|cout|#include <iostream>)\b',
            "csharp": r'\b(c#|csharp|c sharp|in c#|using c#|Console\.WriteLine|namespace)\b',
            "typescript": r'\b(typescript|ts|in ts|using typescript|\.ts\b|interface\s+\w+|type\s+\w+)\b',
            "rust": r'\b(rust|in rust|using rust|cargo)\b',
            "javascript": r'\b(javascript|js|in js|using javascript|node\.?js|console\.log|console log)\b',
            "python": r'\b(python|in python|using python|py|print\(|def )\b',
            "go": r'\b(golang|go lang|in go|using go|fmt\.print)\b',
            "java": r'\b(java|in java|using java|system\.out)\b'
        }

        prompt_lower = prompt.lower()
        for lang, pattern in lang_patterns.items():
            if re.search(pattern, prompt_lower):
                detected_lang = lang
                break

        if st.session_state.code_pending_auto and st.session_state.code_force_language:
            detected_lang = st.session_state.code_force_language

        st.info(f"ğŸ”¤ Detected language: **{detected_lang}**")

        # æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„å·¥å…·é“¾
        if detected_lang == "rust":
            st.markdown("**ğŸ”§ Checking Rust toolchain...**")
            check_result = subprocess.run(['which', 'cargo'], capture_output=True)

            if check_result.returncode != 0:
                st.warning("âš ï¸ Rust toolchain not found. Installing...")

                with st.spinner("ğŸ“¦ Installing Rust (this may take 2-3 minutes)..."):
                    install_cmd = 'curl --proto "=https" --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --default-toolchain stable'
                    install_result = subprocess.run(
                        install_cmd,
                        shell=True,
                        capture_output=True,
                        text=True,
                        timeout=300
                    )

                    if install_result.returncode == 0:
                        st.success("âœ… Rust installed successfully!")
                        # Update PATH
                        os.environ['PATH'] = f"{os.path.expanduser('~/.cargo/bin')}:{os.environ.get('PATH', '')}"
                    else:
                        st.error(f"âŒ Failed to install Rust: {install_result.stderr}")
                        st.stop()
            else:
                st.success("âœ… Rust toolchain ready")

        # ä¼ªæµå¼è¿›åº¦æ˜¾ç¤º
        progress_bar = st.progress(0)
        status_text = st.empty()

        stages = [
            ("ğŸ”¨ Generating initial code", 0.2),
            ("â³ Waiting for LLM response", 0.4),
            ("âœ… Code generated", 0.5),
            ("ğŸ§ª Running tests", 0.7),
            ("â³ Executing test framework", 0.85),
        ]

        try:
            # å¼€å§‹è¿›åº¦åŠ¨ç”»
            for stage_text, progress in stages:
                status_text.text(stage_text)
                progress_bar.progress(progress)
                time.sleep(0.3)

            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()

            # å®é™… API è°ƒç”¨
            print(f"[Code] Calling /api/code/generate with language={detected_lang}")
            response = requests.post(
                f"{BACKEND_URL}/api/code/generate",
                json={
                    "task": prompt,
                    "language": detected_lang,
                    "max_retries": 3,
                    "include_samples": bool(st.session_state.code_show_samples),
                },
                timeout=120
            )
            print(f"[Code] Response status={response.status_code}, bytes={len(response.content)}")

            # è®¡ç®—å»¶è¿Ÿ
            latency_ms = (time.time() - start_time) * 1000

            progress_bar.progress(1.0)
            status_text.text("âœ… Done!")

            if response.status_code == 200:
                result = response.json()

                # Save request for re-run (save immediately after successful API call)
                st.session_state.code_last_request = {
                    "prompt": prompt,
                    "language": detected_lang,
                    "include_samples": bool(st.session_state.code_show_samples),
                }

                # æ˜¾ç¤ºçŠ¶æ€
                if result.get("test_passed"):
                    st.success("âœ… All tests passed!")
                else:
                    st.error("âŒ Tests failed after max retries")

                # æ˜¾ç¤ºå…ƒæ•°æ®
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("Language", result.get('language', 'python'))
                col2.metric("Retries", f"{result.get('total_retries', 0)}/{result.get('max_retries', 3)}")
                col3.metric("Time", f"{result.get('generation_time_ms', 0):.0f}ms")
                col4.metric("Tokens", result.get('tokens_used', 0))

                initial_plan_summary = result.get("initial_plan_summary")
                initial_plan_steps = result.get("initial_plan_steps") or []
                if initial_plan_summary or initial_plan_steps:
                    st.markdown("**ğŸ§  Initial Plan:**")
                    if initial_plan_summary:
                        st.markdown(f"- {initial_plan_summary}")
                    if initial_plan_steps:
                        st.markdown("\n".join([f"  {idx + 1}. {step}" for idx, step in enumerate(initial_plan_steps)]))

                # æ˜¾ç¤ºç”Ÿæˆçš„ä»£ç 
                st.markdown("**Generated Code:**")
                st.code(result.get("code", ""), language=result.get('language', 'python'))

                final_test = result.get("final_test_result", {})

                with st.expander("ğŸ“Š Execution Output", expanded=False):
                    col_a, col_b = st.columns(2)
                    col_a.metric("Exit Code", final_test.get('exit_code', 'N/A'))
                    col_b.metric("Execution Time", f"{final_test.get('execution_time_ms', 0):.0f}ms")

                    stdout = clean_text_lines(final_test.get("stdout", ""))
                    if stdout:
                        st.markdown("**Program Output (print, logs, etc.):**")
                        st.code(stdout, language="text")
                    else:
                        st.info("No program output (no print statements or logs)")

                    stderr = clean_text_lines(final_test.get("stderr", ""))
                    if stderr:
                        st.markdown("**Errors/Warnings:**")
                        st.code(stderr, language="text")

                    samples = final_test.get("samples") or result.get("samples")
                    if samples:
                        st.markdown("**Sample Evaluations:**")
                        for sample in samples:
                            expr = sample.get("expression", "<expression>")
                            actual = sample.get("actual")
                            expected = sample.get("expected")
                            line = f"{expr} â†’ {actual}"
                            if expected is not None:
                                line += f" (expected {expected})"
                            st.markdown(f"- {line}")

                retries = result.get("retry_attempts", [])
                if retries:
                    with st.expander("ğŸ”§ Self-Healing History", expanded=False):
                        for retry in retries:
                            st.markdown(f"**Attempt {retry.get('attempt_number')}:**")
                            st.markdown(f"- Fix Applied: {retry.get('fix_applied', 'N/A')}")
                            root_cause = retry.get('error_analysis')
                            if root_cause:
                                st.markdown(f"- Root Cause: {root_cause}")
                            plan_summary = retry.get('plan_summary')
                            if plan_summary:
                                st.markdown(f"- Plan Overview: {plan_summary}")
                            plan_steps = retry.get('plan_steps') or []
                            if plan_steps:
                                st.markdown("- Plan Steps:")
                                st.markdown("\n".join([f"{idx + 1}. {step}" for idx, step in enumerate(plan_steps)]))
                            retry_test = retry.get("test_result") or {}
                            st.markdown(f"- Exit Code: {retry_test.get('exit_code', 'N/A')}")
                            retry_stdout = clean_text_lines(retry_test.get("stdout", ""))
                            if retry_stdout:
                                st.markdown("- Output:")
                                st.code(retry_stdout, language="text")
                            retry_stderr = clean_text_lines(retry_test.get("stderr", ""))
                            if retry_stderr:
                                st.markdown("- Error:")
                                st.code(retry_stderr, language="text")
                            samples_retry = retry_test.get("samples") or []
                            if samples_retry:
                                st.markdown("- Samples:")
                                for sample in samples_retry:
                                    expr = sample.get("expression", "<expression>")
                                    actual = sample.get("actual")
                                    expected = sample.get("expected")
                                    line = f"{expr} â†’ {actual}"
                                    if expected is not None:
                                        line += f" (expected {expected})"
                                    st.markdown(f"  â€¢ {line}")

                st.markdown("\n**Continue generating code or type 'q'(quit) to exit Code mode.**")

                # æ”¶é›†CodeæŒ‡æ ‡åˆ°dashboard
                st.session_state.metrics_history["timestamps"].append(datetime.now())
                st.session_state.metrics_history["latencies"].append(latency_ms)
                st.session_state.metrics_history["prompt_tokens"].append(result.get('tokens_used', 0) // 2)  # ä¼°ç®—
                st.session_state.metrics_history["completion_tokens"].append(result.get('tokens_used', 0) // 2)
                st.session_state.metrics_history["costs"].append(float(result.get('cost_usd', 0) or 0.0))
                st.session_state.metrics_history["services"].append("code")

                code_stats = st.session_state.code_stats
                code_stats["total_runs"] += 1
                code_stats["total_latency_ms"] += latency_ms
                tokens_used = result.get('tokens_used', 0)
                cost_usd = float(result.get('cost_usd', 0) or 0.0)
                code_stats["total_tokens"] += tokens_used
                code_stats["total_cost_usd"] += cost_usd
                if result.get('test_passed'):
                    code_stats["passes"] += 1
                    status_label = "success"
                else:
                    code_stats["failures"] += 1
                    status_label = "failed_tests"

                history_entry = {
                    "timestamp": datetime.now(),
                    "status": status_label,
                    "test_passed": bool(result.get('test_passed')),
                    "latency_ms": float(latency_ms),
                    "tokens": int(tokens_used),
                    "cost_usd": cost_usd,
                    "language": result.get('language'),
                    "exit_code": final_test.get('exit_code', 'N/A'),
                }
                if stdout:
                    history_entry["stdout"] = stdout
                if stderr:
                    history_entry["stderr"] = stderr
                code_stats["history"].append(history_entry)
                if len(code_stats["history"]) > 10:
                    code_stats["history"].pop(0)

                summary_sections: List[str] = []
                status_text = "Passed" if result.get('test_passed') else "Failed"
                summary_sections.append(f"**Status:** {status_text}")
                summary_sections.append(
                    f"**Metadata:** Language {result.get('language', 'python')} â€¢ Retries {result.get('total_retries', 0)}/{result.get('max_retries', 3)} "
                    f"â€¢ Time {result.get('generation_time_ms', 0):.0f} ms â€¢ Tokens {result.get('tokens_used', 0)} "
                    f"â€¢ Token Cost ${result.get('cost_usd', 0.0):.4f}"
                )

                if initial_plan_summary or initial_plan_steps:
                    plan_text = ""
                    if initial_plan_summary:
                        plan_text += f"Summary: {initial_plan_summary}\n"
                    if initial_plan_steps:
                        plan_text += "\n".join(
                            [f"{idx + 1}. {step}" for idx, step in enumerate(initial_plan_steps)]
                        )
                    summary_sections.append(f"**Initial Plan:**\n{plan_text.strip()}")

                generated_code = result.get("code", "")
                if generated_code:
                    summary_sections.append(
                        f"**Generated Code:**\n```{result.get('language', 'python')}\n{generated_code}\n```"
                    )

                if stdout:
                    summary_sections.append(
                        f"**Program Output:**\n```\n{stdout}\n```"
                    )
                else:
                    summary_sections.append("**Program Output:** _no output_")

                if stderr:
                    summary_sections.append(
                        f"**Errors/Warnings:**\n```\n{stderr}\n```"
                    )

                samples = final_test.get("samples") or result.get("samples") or []
                if samples:
                    sample_lines = []
                    for sample in samples:
                        expr = sample.get("expression", "<expression>")
                        actual = sample.get("actual")
                        expected = sample.get("expected")
                        line = f"{expr} â†’ {actual}"
                        if expected is not None:
                            line += f" (expected {expected})"
                        sample_lines.append(line)
                    summary_sections.append(
                        "**Sample Evaluations:**\n" + "\n".join(f"- {line}" for line in sample_lines)
                    )

                if retries:
                    retry_lines: List[str] = []
                    for retry in retries:
                        line_parts = [f"Attempt {retry.get('attempt_number')}: {retry.get('fix_applied', 'N/A')}"]
                        root_cause = retry.get("error_analysis")
                        if root_cause:
                            line_parts.append(f"Root Cause: {root_cause}")
                        plan_summary_retry = retry.get("plan_summary")
                        if plan_summary_retry:
                            line_parts.append(f"Plan: {plan_summary_retry}")
                        plan_steps_retry = retry.get("plan_steps") or []
                        if plan_steps_retry:
                            numbered = " | ".join(
                                [f"{idx + 1}. {step}" for idx, step in enumerate(plan_steps_retry)]
                            )
                            line_parts.append(f"Steps: {numbered}")
                        test_result_retry = retry.get("test_result") or {}
                        exit_code_retry = test_result_retry.get("exit_code", "N/A")
                        line_parts.append(f"Exit Code: {exit_code_retry}")
                        stdout_retry = clean_text_lines(test_result_retry.get("stdout", ""))
                        stderr_retry = clean_text_lines(test_result_retry.get("stderr", ""))
                        if stdout_retry:
                            line_parts.append(f"Output: {stdout_retry}")
                        if stderr_retry:
                            line_parts.append(f"Error: {stderr_retry}")
                        samples_retry = test_result_retry.get("samples") or []
                        if samples_retry:
                            rendered = []
                            for sample in samples_retry:
                                expr = sample.get("expression", "<expression>")
                                actual = sample.get("actual")
                                expected = sample.get("expected")
                                frag = f"{expr} â†’ {actual}"
                                if expected is not None:
                                    frag += f" (expected {expected})"
                                rendered.append(frag)
                            line_parts.append("Samples: " + " | ".join(rendered))
                        retry_lines.append("; ".join(line_parts))
                    if retry_lines:
                        summary_sections.append("**Self-Healing History:**\n" + "\n".join(f"- {line}" for line in retry_lines))

                append_chat_history("assistant", "\n\n".join(summary_sections))

                # Reset auto-run flags
                st.session_state.code_pending_auto = False
                st.session_state.code_force_language = None

            else:
                error_msg = f"âŒ Error: {response.status_code}\n{response.text}"
                st.error(error_msg)
                code_stats = st.session_state.code_stats
                code_stats["errors"] += 1
                code_stats["history"].append({
                    "timestamp": datetime.now(),
                    "status": "error",
                    "message": error_msg,
                })
                if len(code_stats["history"]) > 10:
                    code_stats["history"].pop(0)
                append_chat_history("assistant", error_msg)
                st.session_state.code_pending_auto = False
                st.session_state.code_force_language = None

        except Exception as e:
            error_msg = f"âŒ Request failed: {e}"
            st.error(error_msg)
            code_stats = st.session_state.code_stats
            code_stats["errors"] += 1
            code_stats["history"].append({
                "timestamp": datetime.now(),
                "status": "exception",
                "message": str(e),
            })
            if len(code_stats["history"]) > 10:
                code_stats["history"].pop(0)
            append_chat_history("assistant", error_msg)
            st.session_state.code_pending_auto = False
            st.session_state.code_force_language = None

# =====================================================================
# è‡ªåŠ¨æ„å›¾è¯†åˆ«ï¼ˆæ—  mode æ—¶ï¼‰- ä½¿ç”¨ LLM åˆ†ææ„å›¾
# =====================================================================

elif current_mode == "general" and prompt and prompt != "__MODE_ACTIVATED__":
    st.sidebar.success("âœ… ENTERED GENERAL BRANCH!")
    print(f"ğŸ’¬ ENTERING GENERAL AI BRANCH")
    debug_log(f"[general] branch start mode={st.session_state.mode} prompt={prompt!r}")
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” Understanding your request..."):
            try:
                from openai import OpenAI

                api_key = os.getenv("OPENAI_API_KEY")
                if not api_key:
                    # â€”â€” æœ¬åœ°å…œåº•ï¼šä¸ç»™ OpenAI ä¹Ÿèƒ½å› â€”â€” #
                    st.info("ğŸ’¬ General AI Assistant Mode (local fallback)")
                    local_reply = (
                        "Hi I got it without API ğŸ‘‹\n\n"
      
                    )
                    st.markdown(local_reply)
                    append_chat_history("assistant", local_reply)
                else:
                    client = OpenAI(
                        api_key=api_key,
                        base_url=os.getenv("OPENAI_BASE_URL")
                    )

                    # ä½¿ç”¨LLMåˆ†ææ„å›¾
                    intent_prompt = f"""Analyze the user's request and classify it into one of these categories:

User request: "{prompt}"

Categories:
- "rag": Questions about documents, books, asking who/what someone or something is, requesting explanations about topics
- "trip": Trip planning, travel arrangements, flights, hotels, destinations, vacation planning
- "code": Code generation, writing functions, implementing algorithms, programming tasks
- "general": General questions, greetings, casual conversation, anything else not matching above

Respond with ONLY ONE WORD: rag, trip, code, or general"""

                    intent_messages = sanitize_messages([
                        {"role": "user", "content": intent_prompt}
                    ])

                    intent_response = client.chat.completions.create(
                        model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo-0125"),
                        messages=intent_messages,
                        temperature=0,
                        max_tokens=10
                    )

                    intent = intent_response.choices[0].message.content.strip().lower()
                    print(f"[Intent] classified intent={intent} for prompt={prompt!r}")

                    # æ ¹æ®æ„å›¾è·¯ç”±
                    if intent == "rag":
                        st.session_state.mode = "rag"
                        st.session_state.pending_prompt = prompt
                        st.rerun()

                    elif intent == "trip":
                        st.session_state.mode = "trip"
                        st.session_state.pending_prompt = prompt
                        st.rerun()

                    elif intent == "code":
                        st.session_state.mode = "code"
                        st.session_state.pending_prompt = prompt
                        st.rerun()

                    else:
                        # General Assistant æ¨¡å¼ - ç›´æ¥ç”¨LLMå›ç­”
                        st.info("ğŸ’¬ General AI Assistant Mode")

                        # æ„å»ºå¯¹è¯å†å²ï¼ˆæœ€è¿‘5æ¡ï¼‰
                        # Clean messages first - remove any with null/empty content
                        recent_messages = [
                            msg for msg in (st.session_state.messages[-10:] if len(st.session_state.messages) > 0 else [])
                            if msg.get("content") and str(msg.get("content")).strip()  # Skip null, empty, or whitespace-only
                        ]
                        conversation_history = [
                            {"role": msg["role"], "content": str(msg["content"])[:200]}  # é™åˆ¶é•¿åº¦ï¼Œç¡®ä¿å­—ç¬¦ä¸²
                            for msg in recent_messages
                        ]

                        # æ·»åŠ ç³»ç»Ÿæç¤º
                        system_msg = {
                            "role": "system",
                            "content": """You are a helpful AI assistant. You provide three specialized services:
- ğŸ“š RAG Q&A: Answer questions about documents in the knowledge base
- âœˆï¸ Trip Planning: Help plan trips with flights, hotels, and itineraries
- ğŸ’» Code Generation: Generate code with automated tests

For general questions not related to these services, provide helpful, concise answers.
If the user asks about trip planning, documents, or code, suggest they use the specialized services."""
                        }

                        messages = [system_msg] + conversation_history + [{"role": "user", "content": prompt}]
                        sanitized_messages = sanitize_messages(messages)

                        model_name = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo-0125")
                        request_start = time.time()
                        assistant_response = client.chat.completions.create(
                            model=model_name,
                            messages=sanitized_messages,
                            temperature=0.7,
                            max_tokens=500
                        )
                        latency_ms = (time.time() - request_start) * 1000

                        answer = assistant_response.choices[0].message.content

                        usage = getattr(assistant_response, "usage", None)
                        prompt_tokens = getattr(usage, "prompt_tokens", 0) if usage else 0
                        completion_tokens = getattr(usage, "completion_tokens", 0) if usage else 0
                        cost_usd = estimate_completion_cost(model_name, prompt_tokens, completion_tokens)

                        # æ˜¾ç¤ºå›å¤
                        st.markdown(answer)

                        # æ˜¾ç¤ºæœåŠ¡æç¤º
                        st.markdown("\n---")
                        st.markdown("**ğŸ’¡ Tip:** For specialized tasks, try:")
                        col1, col2, col3 = st.columns(3)
                        col1.markdown("ğŸ“š RAG Q&A")
                        col2.markdown("âœˆï¸ Trip Planning")
                        col3.markdown("ğŸ’» Code Gen")

                        metrics_cols = st.columns(4)
                        metrics_cols[0].metric("Latency", f"{latency_ms:.0f} ms")
                        metrics_cols[1].metric("Prompt Tokens", prompt_tokens)
                        metrics_cols[2].metric("Completion Tokens", completion_tokens)
                        metrics_cols[3].metric("Token Cost", f"${cost_usd:.4f}")

                        st.session_state.metrics_history["timestamps"].append(datetime.now())
                        st.session_state.metrics_history["latencies"].append(latency_ms)
                        st.session_state.metrics_history["prompt_tokens"].append(prompt_tokens)
                        st.session_state.metrics_history["completion_tokens"].append(completion_tokens)
                        st.session_state.metrics_history["costs"].append(cost_usd)
                        st.session_state.metrics_history["services"].append("chat")

                        # ä¿å­˜åˆ°å†å²
                        append_chat_history(
                            "assistant",
                            answer + f"\n\n---\nLatency: {latency_ms:.0f} ms | Tokens: {prompt_tokens + completion_tokens} | Token Cost: ${cost_usd:.4f}"
                        )

            except Exception as e:
                error_msg = f"âŒ Error: {e}"
                st.error(error_msg)
                append_chat_history("assistant", error_msg)

# Catch-all: if no branch matched
else:
    if prompt:
        st.sidebar.error(f"âš ï¸ NO BRANCH MATCHED! mode={current_mode!r}, prompt={prompt!r}")
