"""
Inference service powered by ONNX Runtime for faster performance.

Compared with the PyTorch implementation:
- Embedding latency: 50-100 ms -> 20-50 ms (roughly 2-5x faster)
- Rerank latency: 150-300 ms -> 50-100 ms (roughly 3-6x faster)
"""
import os
import time
from contextlib import asynccontextmanager
from typing import List, Optional
import numpy as np

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import uvicorn

from inference_service.config import config

# Global ONNX sessions
embedding_session = None
rerank_session = None


# === Pydantic Models (same as before) ===
class EmbedRequest(BaseModel):
    texts: List[str] = Field(..., description="Texts to embed")
    model: Optional[str] = Field(None, description="Embedding model name")
    normalize: bool = Field(True, description="Whether to L2-normalize vectors")
    batch_size: Optional[int] = Field(None, description="Batch size override")


class EmbedResponse(BaseModel):
    embeddings: List[List[float]]
    model: str
    dimension: int
    processing_time_ms: float
    batch_info: dict


class RerankRequest(BaseModel):
    query: str = Field(..., description="Search query text")
    documents: List[str] = Field(..., description="Documents to be reranked")
    model: Optional[str] = Field(None, description="Reranker model name")
    top_k: Optional[int] = Field(None, description="Return top-k results")
    return_documents: bool = Field(False, description="Include document text in response")


class RerankResponse(BaseModel):
    scores: List[float]
    indices: Optional[List[int]] = None
    documents: Optional[List[str]] = None
    processing_time_ms: float


class HealthResponse(BaseModel):
    status: str
    uptime_seconds: float
    models_loaded: dict
    config: dict


# === ONNX Runtime Wrapper ===
class ONNXEmbeddingModel:
    """ONNX Runtime wrapper for embedding models"""

    def __init__(self, model_path: str, device: str = "cpu"):
        try:
            import onnxruntime as ort
            from transformers import AutoTokenizer
        except ImportError:
            raise ImportError(
                "Please install: pip install onnxruntime transformers"
            )

        self.tokenizer = AutoTokenizer.from_pretrained(
            os.path.dirname(model_path) if os.path.isfile(model_path) else model_path
        )

        # Configure execution providers
        providers = []
        if device.startswith("cuda"):
            providers.append("CUDAExecutionProvider")
        providers.append("CPUExecutionProvider")

        # Session options with CPU optimizations
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        # Configure thread pools based on available CPU cores
        import os as _os
        num_threads = int(_os.getenv("OMP_NUM_THREADS", "6"))
        sess_options.intra_op_num_threads = num_threads
        sess_options.inter_op_num_threads = num_threads

        onnx_file = os.path.join(model_path, "model.onnx") if os.path.isdir(model_path) else model_path
        self.session = ort.InferenceSession(onnx_file, sess_options=sess_options, providers=providers)

        print(f"‚úÖ ONNX model loaded: {onnx_file}")
        print(f"   Providers: {self.session.get_providers()}")
        print(f"   CPU Threads: {num_threads}")

    def encode(self, texts: List[str], batch_size: int = 32, max_length: int = 512):
        """Generate embeddings using ONNX Runtime"""
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            # Tokenize
            encoded = self.tokenizer(
                batch,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="np"
            )

            # Run ONNX inference
            ort_inputs = {
                "input_ids": encoded["input_ids"].astype(np.int64),
                "attention_mask": encoded["attention_mask"].astype(np.int64)
            }

            if "token_type_ids" in encoded:
                ort_inputs["token_type_ids"] = encoded["token_type_ids"].astype(np.int64)

            outputs = self.session.run(None, ort_inputs)

            # Extract embeddings (mean pooling)
            embeddings = self._mean_pooling(outputs[0], encoded["attention_mask"])
            all_embeddings.append(embeddings)

        return np.vstack(all_embeddings)

    def _mean_pooling(self, token_embeddings, attention_mask):
        """Mean pooling to get sentence embeddings"""
        input_mask_expanded = np.expand_dims(attention_mask, -1).astype(float)
        sum_embeddings = np.sum(token_embeddings * input_mask_expanded, axis=1)
        sum_mask = np.clip(input_mask_expanded.sum(axis=1), a_min=1e-9, a_max=None)
        return sum_embeddings / sum_mask


class ONNXRerankerModel:
    """ONNX Runtime wrapper for reranker models"""

    def __init__(self, model_path: str, device: str = "cpu"):
        try:
            import onnxruntime as ort
            from transformers import AutoTokenizer
        except ImportError:
            raise ImportError(
                "Please install: pip install onnxruntime transformers"
            )

        self.tokenizer = AutoTokenizer.from_pretrained(
            os.path.dirname(model_path) if os.path.isfile(model_path) else model_path
        )

        # Configure execution providers
        providers = []
        if device.startswith("cuda"):
            providers.append("CUDAExecutionProvider")
        providers.append("CPUExecutionProvider")

        # Session options with CPU optimizations
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

        # Configure thread pools
        import os as _os
        num_threads = int(_os.getenv("OMP_NUM_THREADS", "6"))
        sess_options.intra_op_num_threads = num_threads
        sess_options.inter_op_num_threads = num_threads

        onnx_file = os.path.join(model_path, "model.onnx") if os.path.isdir(model_path) else model_path
        self.session = ort.InferenceSession(onnx_file, sess_options=sess_options, providers=providers)

        print(f"‚úÖ ONNX reranker loaded: {onnx_file}")
        print(f"   Providers: {self.session.get_providers()}")
        print(f"   CPU Threads: {num_threads}")

    def compute_score(self, pairs: List[List[str]], batch_size: int = 64):
        """Compute rerank scores using ONNX Runtime"""
        all_scores = []

        for i in range(0, len(pairs), batch_size):
            batch_pairs = pairs[i:i + batch_size]

            # Tokenize pairs
            encoded = self.tokenizer(
                batch_pairs,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="np"
            )

            # Run ONNX inference
            ort_inputs = {
                "input_ids": encoded["input_ids"].astype(np.int64),
                "attention_mask": encoded["attention_mask"].astype(np.int64)
            }

            if "token_type_ids" in encoded:
                ort_inputs["token_type_ids"] = encoded["token_type_ids"].astype(np.int64)

            outputs = self.session.run(None, ort_inputs)

            # Extract scores from logits
            # Output shape: (batch_size, 1) for classification model
            logits = outputs[0]

            # Squeeze to get (batch_size,)
            if len(logits.shape) > 1 and logits.shape[-1] == 1:
                scores = logits.squeeze(-1)  # (batch_size, 1) -> (batch_size,)
            else:
                scores = logits.flatten()  # Ensure 1D array

            # Convert to list
            all_scores.extend(scores.tolist())

        return all_scores


# === Application Lifespan ===
startup_time = time.time()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifecycle management."""
    global embedding_session, rerank_session

    print("=" * 60)
    print("üöÄ Starting ONNX Inference Service")
    print("=" * 60)
    print(f"Config: {config.get_config_summary()}")
    print("-" * 60)

    # Load ONNX models
    try:
        embed_model_path = os.getenv("ONNX_EMBED_MODEL_PATH", "./models/bge-m3-onnx")
        rerank_model_path = os.getenv("ONNX_RERANK_MODEL_PATH", "./models/bge-reranker-onnx")

        # Allow INT8-quantized models
        use_int8 = os.getenv("USE_INT8_QUANTIZATION", "true").lower() == "true"
        if use_int8:
            # Check for INT8 embedding model
            int8_embed_path = os.path.join(embed_model_path, "model_int8.onnx") if os.path.isdir(embed_model_path) else embed_model_path.replace(".onnx", "_int8.onnx")
            if os.path.exists(int8_embed_path):
                print(f"üöÄ Using INT8 quantized embedding: {int8_embed_path}")
                embed_model_path = int8_embed_path
            else:
                print(f"‚ö†Ô∏è  Embedding INT8 model not found: {int8_embed_path}, using FP32 model")

            # Check for INT8 reranker model
            int8_rerank_path = os.path.join(rerank_model_path, "model_int8.onnx") if os.path.isdir(rerank_model_path) else rerank_model_path.replace(".onnx", "_int8.onnx")
            if os.path.exists(int8_rerank_path):
                print(f"üöÄ Using INT8 quantized reranker: {int8_rerank_path}")
                rerank_model_path = int8_rerank_path
            else:
                print(f"‚ö†Ô∏è  Reranker INT8 model not found: {int8_rerank_path}, using FP32 model")

        print(f"\nüì¶ Loading ONNX embedding model: {embed_model_path}")
        embedding_session = ONNXEmbeddingModel(embed_model_path, device=config.DEVICE)

        print(f"\nüì¶ Loading ONNX reranker model: {rerank_model_path}")
        rerank_session = ONNXRerankerModel(rerank_model_path, device=config.DEVICE)

        # Warm up both models
        print("\nüî• Warming up models...")
        _ = embedding_session.encode(["warm up text"])
        _ = rerank_session.compute_score([["warm up query", "warm up doc"]])
        print("‚úÖ Models warmed up")

    except Exception as e:
        print(f"\n‚ùå Error loading ONNX models: {e}")
        print("   Fallback: Using PyTorch models instead...")

        # Fallback to PyTorch
        try:
            from FlagEmbedding import BGEM3FlagModel, FlagReranker
            embedding_session = BGEM3FlagModel(config.EMBED_MODEL_NAME, device=config.DEVICE)
            rerank_session = FlagReranker(config.RERANK_MODEL_NAME, device=config.DEVICE)
            print("‚úÖ PyTorch models loaded as fallback")
        except:
            pass

    print("\n" + "=" * 60)
    print("‚úÖ Inference Service Ready!")
    print("=" * 60)

    yield

    # Teardown
    print("\nüßπ Shutting down...")


# === FastAPI App ===
app = FastAPI(
    title="ONNX Inference Service",
    description="High-performance inference service powered by ONNX Runtime",
    version="0.2.0",
    lifespan=lifespan
)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    uptime = time.time() - startup_time
    return HealthResponse(
        status="healthy" if embedding_session and rerank_session else "degraded",
        uptime_seconds=uptime,
        models_loaded={
            "embedding": embedding_session is not None,
            "rerank": rerank_session is not None
        },
        config=config.get_config_summary()
    )


@app.post("/embed", response_model=EmbedResponse)
async def embed_texts_api(request: EmbedRequest):
    """Generate text embeddings with ONNX acceleration."""
    if embedding_session is None:
        raise HTTPException(status_code=503, detail="Embedding model not loaded")

    if not request.texts:
        raise HTTPException(status_code=400, detail="texts cannot be empty")

    start_time = time.perf_counter()

    try:
        embeddings = embedding_session.encode(
            request.texts,
            batch_size=request.batch_size or config.MAX_BATCH_SIZE
        )

        # Normalize if requested
        if request.normalize:
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings = embeddings / norms

        duration = (time.perf_counter() - start_time) * 1000

        return EmbedResponse(
            embeddings=embeddings.tolist(),
            model=request.model or "bge-m3-onnx",
            dimension=embeddings.shape[1],
            processing_time_ms=round(duration, 2),
            batch_info={
                "total_texts": len(request.texts),
                "batch_size": request.batch_size or config.MAX_BATCH_SIZE
            }
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Embedding failed: {str(e)}")


@app.post("/rerank", response_model=RerankResponse)
async def rerank_documents_api(request: RerankRequest):
    """Rerank documents with ONNX acceleration."""
    if rerank_session is None:
        raise HTTPException(status_code=503, detail="Rerank model not loaded")

    if not request.documents:
        raise HTTPException(status_code=400, detail="documents cannot be empty")

    start_time = time.perf_counter()

    try:
        pairs = [[request.query, doc] for doc in request.documents]
        scores = rerank_session.compute_score(pairs, batch_size=64)

        # Sort scores and keep top-k
        scored_docs = list(enumerate(scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        top_k = request.top_k or len(request.documents)
        scored_docs = scored_docs[:top_k]

        indices = [idx for idx, score in scored_docs]
        sorted_scores = [score for idx, score in scored_docs]

        duration = (time.perf_counter() - start_time) * 1000

        result = RerankResponse(
            scores=sorted_scores,
            indices=indices,
            processing_time_ms=round(duration, 2)
        )

        if request.return_documents:
            result.documents = [request.documents[i] for i in indices]

        return result

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Rerank failed: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(
        "inference_service.main_onnx:app",
        host=config.HOST,
        port=config.PORT,
        workers=config.WORKERS
    )
