version: '3.8'

services:
  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__STORAGE__IN_MEMORY=false
      - QDRANT__STORAGE__STORAGE_PATH=/qdrant/storage
      - QDRANT__STORAGE__WRITE_CONSISTENCY_FACTOR=1
    restart: unless-stopped

  # ONNX Inference Service (remote)
  inference:
    build:
      context: .
      dockerfile: inference/Dockerfile
    container_name: inference-service
    ports:
      - "8001:8001"
    volumes:
      - ./models:/app/models:ro
    environment:
      - DEVICE=cpu
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-6}
      - ONNX_EMBED_MODEL_PATH=/app/models/minilm-embed-int8
      - ONNX_RERANK_MODEL_PATH=/app/models/minilm-reranker-onnx
      - USE_INT8_QUANTIZATION=${USE_INT8_QUANTIZATION:-true}
    restart: unless-stopped

  # Backend API (FastAPI)
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend-api
    env_file:
      - .env
    ports:
      - "8888:8888"
    volumes:
      - ./models:/app/models:ro
      - ./data:/app/data
      - ./chat_sessions.sqlite3:/app/chat_sessions.sqlite3
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION=${QDRANT_COLLECTION:-assessment_docs_minilm}
      - ONNX_EMBED_MODEL_PATH=/app/models/minilm-embed-int8
      - ONNX_RERANK_MODEL_PATH=/app/models/minilm-reranker-onnx
      - EMBED_FALLBACK_MODEL_PATH=/app/models/minilm-embed-int8
      - RERANK_FALLBACK_MODEL_PATH=/app/models/minilm-reranker-onnx
      - ENABLE_REMOTE_INFERENCE=false
      - USE_ONNX_INFERENCE=${USE_ONNX_INFERENCE:-true}
      - OPENAI_API_KEY=sk-zM2MxgXhXnusmExHx1sKyw
      - OPENAI_BASE_URL=https://aiunifier.wonderfulrock-83cb33fd.australiaeast.azurecontainerapps.io
      - OPENAI_MODEL=Gpt4o
      - QDRANT_SEED_PATH=${QDRANT_SEED_PATH}
      - QDRANT_SEED_VECTOR_SIZE=${QDRANT_SEED_VECTOR_SIZE:-384}
      - QDRANT_SEED_TARGET_COUNT=${QDRANT_SEED_TARGET_COUNT:-138000}
      - RAG_VECTOR_SIZE=${RAG_VECTOR_SIZE:-384}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_KEY=${AZURE_OPENAI_KEY}
      - AZURE_OPENAI_DEPLOYMENT=${AZURE_OPENAI_DEPLOYMENT}
      - AZURE_OPENAI_API_VERSION=${AZURE_OPENAI_API_VERSION}
      - OMP_NUM_THREADS=${OMP_NUM_THREADS:-16}
      - PYTHONUNBUFFERED=1
    depends_on:
      qdrant:
        condition: service_started
      inference:
        condition: service_started
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 15s
      timeout: 10s
      retries: 3

  # Streamlit Frontend
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: streamlit-ui
    ports:
      - "18501:8501"
    environment:
      - BACKEND_URL=http://backend:8888
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - OPENAI_API_KEY=sk-zM2MxgXhXnusmExHx1sKyw
      - OPENAI_BASE_URL=https://aiunifier.wonderfulrock-83cb33fd.australiaeast.azurecontainerapps.io
      - OPENAI_MODEL=Gpt4o
    depends_on:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 15s
      timeout: 10s
      retries: 3

volumes:
  qdrant_storage:
    driver: local

networks:
  default:
    name: ai-assessment-network
    driver: bridge
